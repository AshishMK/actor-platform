00:54:51.528UTC [34mINFO [0;39m [36ma.c.Cluster(akka://actor-server)[0;39m [35makka.cluster.Cluster(akka://actor-server)[0;39m 
                 Cluster Node [akka.tcp://actor-server@192.168.1.3:2552] - Starting up...
00:54:51.730UTC [34mINFO [0;39m [36ma.c.Cluster(akka://actor-server)[0;39m [35makka.cluster.Cluster(akka://actor-server)[0;39m 
                 Cluster Node [akka.tcp://actor-server@192.168.1.3:2552] - Registered cluster JMX MBean [akka:type=Cluster]
00:54:51.730UTC [34mINFO [0;39m [36ma.c.Cluster(akka://actor-server)[0;39m [35makka.cluster.Cluster(akka://actor-server)[0;39m 
                 Cluster Node [akka.tcp://actor-server@192.168.1.3:2552] - Started up successfully
00:54:51.791UTC [34mINFO [0;39m [36ma.c.Cluster(akka://actor-server)[0;39m [35makka.cluster.Cluster(akka://actor-server)[0;39m 
                 Cluster Node [akka.tcp://actor-server@192.168.1.3:2552] - Metrics will be retreived from MBeans, and may be incorrect on some platforms. To increase metric accuracy add the 'sigar.jar' to the classpath and the appropriate platform-specific native libary to 'java.library.path'. Reason: java.lang.ClassNotFoundException: org.hyperic.sigar.Sigar
00:54:51.808UTC [34mINFO [0;39m [36ma.c.Cluster(akka://actor-server)[0;39m [35makka.cluster.Cluster(akka://actor-server)[0;39m 
                 Cluster Node [akka.tcp://actor-server@192.168.1.3:2552] - Metrics collection has started successfully
00:54:51.844UTC [34mINFO [0;39m [36ma.c.Cluster(akka://actor-server)[0;39m [35makka.cluster.Cluster(akka://actor-server)[0;39m 
                 Cluster Node [akka.tcp://actor-server@192.168.1.3:2552] - Node [akka.tcp://actor-server@192.168.1.3:2552] is JOINING, roles []
00:54:51.886UTC [34mINFO [0;39m [36ma.c.Cluster(akka://actor-server)[0;39m [35makka.cluster.Cluster(akka://actor-server)[0;39m 
                 Cluster Node [akka.tcp://actor-server@192.168.1.3:2552] - Leader is moving node [akka.tcp://actor-server@192.168.1.3:2552] to [Up]
 [31mWARN [0;39m [36mo.f.core.internal.command.DbMigrate[0;39m [35m[0;39m 
                 Schema "public" has a version (20161113105100) that is newer than the latest available migration (20160902182358) !
00:54:53.873UTC [39mDEBUG[0;39m [36ma.c.singleton.ClusterSingletonProxy[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/SimpleKeyValueRoot-Migrations-Proxy[0;39m 
                 Creating singleton identification timer...
00:54:53.887UTC [39mDEBUG[0;39m [36ma.c.singleton.ClusterSingletonProxy[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/SimpleKeyValueRoot-Migrations-Proxy[0;39m 
                 Trying to identify singleton at [akka.tcp://actor-server@192.168.1.3:2552/user/SimpleKeyValueRoot-Migrations/singleton]
00:54:53.995UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/SimpleKeyValueRoot-Migrations[0;39m 
                 Singleton manager starting singleton actor [akka://actor-server/user/SimpleKeyValueRoot-Migrations/singleton]
00:54:54.005UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/SimpleKeyValueRoot-Migrations[0;39m 
                 ClusterSingletonManager state change [Start -> Oldest]
00:54:54.051UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-MigrationsCoordinator[0;39m 
                 Singleton manager starting singleton actor [akka://actor-server/system/sharding/shardakka-kv-MigrationsCoordinator/singleton]
00:54:54.051UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-MigrationsCoordinator[0;39m 
                 ClusterSingletonManager state change [Start -> Oldest]
00:54:54.120UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Request shard [53] home
00:54:54.143UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Get for key [shardakka-kv-MigrationsCoordinatorState], local data [None]
00:54:54.143UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Coordinator moved from [] to [akka.tcp://actor-server@192.168.1.3:2552]
00:54:54.167UTC [34mINFO [0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-MigrationsCoordinator/singleton/coordinator[0;39m 
                 Sharding Coordinator was moved to the active state State(Map(),Map(),Set(),Set(),false)
00:54:54.167UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-MigrationsCoordinator/singleton/coordinator[0;39m 
                 ShardRegion registered: [Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]]
00:54:54.198UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [shardakka-kv-MigrationsCoordinatorState], old data [None], new data [LWWRegister(State(Map(),Map(Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732] -> Vector()),Set(),Set(),false))]
00:54:54.226UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-MigrationsCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardRegionRegistered(Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732])
00:54:54.234UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Retry request for shard [53] homes from coordinator at [Actor[akka://actor-server/system/sharding/shardakka-kv-MigrationsCoordinator/singleton/coordinator#-926250839]]. [1] buffered messages.
00:54:54.262UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [shardakka-kv-MigrationsCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(),Map(Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732] -> Vector()),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(53 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]),Map(Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732] -> Vector(53)),Set(),Set(),false))]
00:54:54.262UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-MigrationsCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(53,Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732])
00:54:54.263UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-MigrationsCoordinator/singleton/coordinator[0;39m 
                 Shard [53] allocated at [Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]]
00:54:54.263UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Host Shard [53] 
00:54:54.273UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Starting shard [53] in region
00:54:54.283UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Shard [53] located at [Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]]
00:54:54.290UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Shard was initialized 53
00:54:54.290UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Deliver [1] buffered messages for shard [53]
00:54:54.294UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations/53[0;39m 
                 Starting entity [2015-08-04-UsersMigration] in shard [53]
00:54:54.877UTC [39mDEBUG[0;39m [36ma.c.singleton.ClusterSingletonProxy[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/SimpleKeyValueRoot-Migrations-Proxy[0;39m 
                 Trying to identify singleton at [akka.tcp://actor-server@192.168.1.3:2552/user/SimpleKeyValueRoot-Migrations/singleton]
00:54:54.881UTC [34mINFO [0;39m [36ma.c.singleton.ClusterSingletonProxy[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/SimpleKeyValueRoot-Migrations-Proxy[0;39m 
                 Singleton identified at [akka://actor-server/user/SimpleKeyValueRoot-Migrations/singleton]
00:54:54.883UTC [39mDEBUG[0;39m [36ma.c.singleton.ClusterSingletonProxy[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/SimpleKeyValueRoot-Migrations-Proxy[0;39m 
                 Sending buffered messages to current singleton instance
00:54:55.146UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Request shard [74] home
00:54:55.147UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [shardakka-kv-MigrationsCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(53 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]),Map(Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732] -> Vector(53)),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(53 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 74 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]),Map(Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732] -> Vector(53, 74)),Set(),Set(),false))]
00:54:55.147UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-MigrationsCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(74,Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732])
00:54:55.147UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-MigrationsCoordinator/singleton/coordinator[0;39m 
                 Shard [74] allocated at [Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]]
00:54:55.147UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Host Shard [74] 
00:54:55.147UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Starting shard [74] in region
00:54:55.148UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Shard [74] located at [Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]]
00:54:55.148UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Shard was initialized 74
00:54:55.148UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Deliver [1] buffered messages for shard [74]
00:54:55.148UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations/74[0;39m 
                 Starting entity [2015-08-04-GroupsMigration] in shard [74]
00:54:55.162UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Request shard [67] home
00:54:55.163UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [shardakka-kv-MigrationsCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(53 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 74 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]),Map(Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732] -> Vector(53, 74)),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(53 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 74 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 67 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]),Map(Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732] -> Vector(53, 74, 67)),Set(),Set(),false))]
00:54:55.163UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-MigrationsCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(67,Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732])
00:54:55.163UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-MigrationsCoordinator/singleton/coordinator[0;39m 
                 Shard [67] allocated at [Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]]
00:54:55.163UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Host Shard [67] 
00:54:55.163UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Starting shard [67] in region
00:54:55.164UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Shard [67] located at [Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]]
00:54:55.164UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Shard was initialized 67
00:54:55.164UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Deliver [1] buffered messages for shard [67]
00:54:55.164UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations/67[0;39m 
                 Starting entity [2015-08-20-LocalNamesMigration] in shard [67]
00:54:55.178UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Request shard [24] home
00:54:55.178UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [shardakka-kv-MigrationsCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(53 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 74 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 67 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]),Map(Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732] -> Vector(53, 74, 67)),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(53 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 74 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 67 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 24 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]),Map(Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732] -> Vector(53, 74, 67, 24)),Set(),Set(),false))]
00:54:55.178UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-MigrationsCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(24,Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732])
00:54:55.178UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-MigrationsCoordinator/singleton/coordinator[0;39m 
                 Shard [24] allocated at [Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]]
00:54:55.178UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Host Shard [24] 
00:54:55.178UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Starting shard [24] in region
00:54:55.179UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Shard [24] located at [Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]]
00:54:55.179UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Shard was initialized 24
00:54:55.179UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Deliver [1] buffered messages for shard [24]
00:54:55.179UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations/24[0;39m 
                 Starting entity [2015-08-29-GroupCreatorMemberMigration] in shard [24]
00:54:55.196UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Request shard [63] home
00:54:55.197UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [shardakka-kv-MigrationsCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(53 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 74 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 67 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 24 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]),Map(Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732] -> Vector(53, 74, 67, 24)),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(67 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 24 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 53 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 63 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 74 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]),Map(Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732] -> Vector(53, 74, 67, 24, 63)),Set(),Set(),false))]
00:54:55.197UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-MigrationsCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(63,Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732])
00:54:55.197UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-MigrationsCoordinator/singleton/coordinator[0;39m 
                 Shard [63] allocated at [Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]]
00:54:55.197UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Host Shard [63] 
00:54:55.197UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Starting shard [63] in region
00:54:55.198UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Shard [63] located at [Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]]
00:54:55.198UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Shard was initialized 63
00:54:55.198UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Deliver [1] buffered messages for shard [63]
00:54:55.198UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations/63[0;39m 
                 Starting entity [PutHiddenGroupsToSQL] in shard [63]
00:54:55.212UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Request shard [93] home
00:54:55.213UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [shardakka-kv-MigrationsCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(67 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 24 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 53 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 63 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 74 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]),Map(Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732] -> Vector(53, 74, 67, 24, 63)),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(67 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 93 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 24 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 53 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 63 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 74 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]),Map(Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732] -> Vector(53, 74, 67, 24, 63, 93)),Set(),Set(),false))]
00:54:55.213UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-MigrationsCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(93,Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732])
00:54:55.213UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-MigrationsCoordinator/singleton/coordinator[0;39m 
                 Shard [93] allocated at [Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]]
00:54:55.213UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Host Shard [93] 
00:54:55.213UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Starting shard [93] in region
00:54:55.214UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Shard [93] located at [Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]]
00:54:55.214UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Shard was initialized 93
00:54:55.214UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Deliver [1] buffered messages for shard [93]
00:54:55.214UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations/93[0;39m 
                 Starting entity [2015-11-03-LocalNamesFromKVMigration] in shard [93]
00:54:55.227UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Request shard [94] home
00:54:55.228UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [shardakka-kv-MigrationsCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(67 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 93 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 24 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 53 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 63 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 74 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]),Map(Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732] -> Vector(53, 74, 67, 24, 63, 93)),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(67 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 93 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 24 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 94 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 53 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 63 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 74 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]),Map(Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732] -> Vector(53, 74, 67, 24, 63, 93, 94)),Set(),Set(),false))]
00:54:55.228UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-MigrationsCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(94,Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732])
00:54:55.228UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-MigrationsCoordinator/singleton/coordinator[0;39m 
                 Shard [94] allocated at [Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]]
00:54:55.228UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Host Shard [94] 
00:54:55.228UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Starting shard [94] in region
00:54:55.228UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Shard [94] located at [Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]]
00:54:55.228UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Shard was initialized 94
00:54:55.229UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Deliver [1] buffered messages for shard [94]
00:54:55.229UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations/94[0;39m 
                 Starting entity [2015-11-11-FillUserSequence] in shard [94]
00:54:55.243UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Request shard [76] home
00:54:55.244UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [shardakka-kv-MigrationsCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(67 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 93 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 24 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 94 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 53 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 63 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 74 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]),Map(Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732] -> Vector(53, 74, 67, 24, 63, 93, 94)),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(67 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 93 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 24 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 94 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 76 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 53 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 63 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 74 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]),Map(Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732] -> Vector(53, 74, 67, 24, 63, 93, 94, 76)),Set(),Set(),false))]
00:54:55.244UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-MigrationsCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(76,Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732])
00:54:55.244UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-MigrationsCoordinator/singleton/coordinator[0;39m 
                 Shard [76] allocated at [Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]]
00:54:55.244UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Host Shard [76] 
00:54:55.244UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Starting shard [76] in region
00:54:55.245UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Shard [76] located at [Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]]
00:54:55.245UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Shard was initialized 76
00:54:55.245UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Deliver [1] buffered messages for shard [76]
00:54:55.245UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations/76[0;39m 
                 Starting entity [2015-11-12-FixUserSequence] in shard [76]
00:54:55.341UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Coordinator moved from [] to [akka.tcp://actor-server@192.168.1.3:2552]
00:54:55.344UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManagerCoordinator[0;39m 
                 Singleton manager starting singleton actor [akka://actor-server/system/sharding/WeakUpdatesManagerCoordinator/singleton]
00:54:55.344UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManagerCoordinator[0;39m 
                 ClusterSingletonManager state change [Start -> Oldest]
00:54:55.344UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Get for key [WeakUpdatesManagerCoordinatorState], local data [None]
00:54:55.345UTC [34mINFO [0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 Sharding Coordinator was moved to the active state State(Map(),Map(),Set(),Set(),false)
00:54:55.351UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManager[0;39m 
                 Coordinator moved from [] to [akka.tcp://actor-server@192.168.1.3:2552]
00:54:55.353UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManagerCoordinator[0;39m 
                 Singleton manager starting singleton actor [akka://actor-server/system/sharding/PresenceManagerCoordinator/singleton]
00:54:55.353UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManagerCoordinator[0;39m 
                 ClusterSingletonManager state change [Start -> Oldest]
00:54:55.354UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Get for key [PresenceManagerCoordinatorState], local data [None]
00:54:55.354UTC [34mINFO [0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManagerCoordinator/singleton/coordinator[0;39m 
                 Sharding Coordinator was moved to the active state State(Map(),Map(),Set(),Set(),false)
00:54:55.362UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/GroupPresenceManager[0;39m 
                 Coordinator moved from [] to [akka.tcp://actor-server@192.168.1.3:2552]
00:54:55.364UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/GroupPresenceManagerCoordinator[0;39m 
                 Singleton manager starting singleton actor [akka://actor-server/system/sharding/GroupPresenceManagerCoordinator/singleton]
00:54:55.364UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/GroupPresenceManagerCoordinator[0;39m 
                 ClusterSingletonManager state change [Start -> Oldest]
00:54:55.365UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Get for key [GroupPresenceManagerCoordinatorState], local data [None]
00:54:55.366UTC [34mINFO [0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/GroupPresenceManagerCoordinator/singleton/coordinator[0;39m 
                 Sharding Coordinator was moved to the active state State(Map(),Map(),Set(),Set(),false)
00:54:55.491UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManager[0;39m 
                 Coordinator moved from [] to [akka.tcp://actor-server@192.168.1.3:2552]
00:54:55.493UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManagerCoordinator[0;39m 
                 Singleton manager starting singleton actor [akka://actor-server/system/sharding/SocialManagerCoordinator/singleton]
00:54:55.493UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManagerCoordinator[0;39m 
                 ClusterSingletonManager state change [Start -> Oldest]
00:54:55.494UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Get for key [SocialManagerCoordinatorState], local data [None]
00:54:55.494UTC [34mINFO [0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManagerCoordinator/singleton/coordinator[0;39m 
                 Sharding Coordinator was moved to the active state State(Map(),Map(),Set(),Set(),false)
00:54:55.525UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessor[0;39m 
                 Coordinator moved from [] to [akka.tcp://actor-server@192.168.1.3:2552]
00:54:55.527UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessorCoordinator[0;39m 
                 Singleton manager starting singleton actor [akka://actor-server/system/sharding/UserProcessorCoordinator/singleton]
00:54:55.527UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessorCoordinator[0;39m 
                 ClusterSingletonManager state change [Start -> Oldest]
00:54:55.528UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Get for key [UserProcessorCoordinatorState], local data [None]
00:54:55.528UTC [34mINFO [0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessorCoordinator/singleton/coordinator[0;39m 
                 Sharding Coordinator was moved to the active state State(Map(),Map(),Set(),Set(),false)
00:54:55.938UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/GroupProcessor[0;39m 
                 Coordinator moved from [] to [akka.tcp://actor-server@192.168.1.3:2552]
00:54:55.940UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/GroupProcessorCoordinator[0;39m 
                 Singleton manager starting singleton actor [akka://actor-server/system/sharding/GroupProcessorCoordinator/singleton]
00:54:55.940UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/GroupProcessorCoordinator[0;39m 
                 ClusterSingletonManager state change [Start -> Oldest]
00:54:55.941UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Get for key [GroupProcessorCoordinatorState], local data [None]
00:54:55.941UTC [34mINFO [0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/GroupProcessorCoordinator/singleton/coordinator[0;39m 
                 Sharding Coordinator was moved to the active state State(Map(),Map(),Set(),Set(),false)
00:54:55.942UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Request shard [1] home
00:54:55.942UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [shardakka-kv-MigrationsCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(67 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 93 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 24 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 94 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 76 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 53 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 63 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 74 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]),Map(Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732] -> Vector(53, 74, 67, 24, 63, 93, 94, 76)),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(67 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 93 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 24 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 94 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 76 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 1 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 53 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 63 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732], 74 -> Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]),Map(Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732] -> Vector(53, 74, 67, 24, 63, 93, 94, 76, 1)),Set(),Set(),false))]
00:54:55.943UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-MigrationsCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(1,Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732])
00:54:55.943UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-MigrationsCoordinator/singleton/coordinator[0;39m 
                 Shard [1] allocated at [Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]]
00:54:55.943UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Host Shard [1] 
00:54:55.943UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Starting shard [1] in region
00:54:55.943UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Shard [1] located at [Actor[akka://actor-server/system/sharding/shardakka-kv-Migrations#-2009674732]]
00:54:55.943UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Shard was initialized 1
00:54:55.944UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations[0;39m 
                 Deliver [1] buffered messages for shard [1]
00:54:55.944UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations/1[0;39m 
                 Starting entity [2015-08-21-IntegrationTokenMigration] in shard [1]
00:54:55.955UTC [31mWARN [0;39m [36makka.actor.ActorSystemImpl[0;39m [35makka.actor.ActorSystemImpl(actor-server)[0;39m 
                 Starting session region
00:54:56.000UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/Session[0;39m 
                 Coordinator moved from [] to [akka.tcp://actor-server@192.168.1.3:2552]
00:54:56.001UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SessionCoordinator[0;39m 
                 Singleton manager starting singleton actor [akka://actor-server/system/sharding/SessionCoordinator/singleton]
00:54:56.001UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SessionCoordinator[0;39m 
                 ClusterSingletonManager state change [Start -> Oldest]
00:54:56.002UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Get for key [SessionCoordinatorState], local data [None]
00:54:56.003UTC [34mINFO [0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SessionCoordinator/singleton/coordinator[0;39m 
                 Sharding Coordinator was moved to the active state State(Map(),Map(),Set(),Set(),false)
00:54:57.357UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 ShardRegion registered: [Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]]
00:54:57.357UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [WeakUpdatesManagerCoordinatorState], old data [None], new data [LWWRegister(State(Map(),Map(Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232] -> Vector()),Set(),Set(),false))]
00:54:57.357UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardRegionRegistered(Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232])
00:54:57.367UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManagerCoordinator/singleton/coordinator[0;39m 
                 ShardRegion registered: [Actor[akka://actor-server/system/sharding/PresenceManager#-2053734734]]
00:54:57.367UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [PresenceManagerCoordinatorState], old data [None], new data [LWWRegister(State(Map(),Map(Actor[akka://actor-server/system/sharding/PresenceManager#-2053734734] -> Vector()),Set(),Set(),false))]
00:54:57.367UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManagerCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardRegionRegistered(Actor[akka://actor-server/system/sharding/PresenceManager#-2053734734])
00:54:57.377UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/GroupPresenceManagerCoordinator/singleton/coordinator[0;39m 
                 ShardRegion registered: [Actor[akka://actor-server/system/sharding/GroupPresenceManager#1573901513]]
00:54:57.378UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [GroupPresenceManagerCoordinatorState], old data [None], new data [LWWRegister(State(Map(),Map(Actor[akka://actor-server/system/sharding/GroupPresenceManager#1573901513] -> Vector()),Set(),Set(),false))]
00:54:57.378UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/GroupPresenceManagerCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardRegionRegistered(Actor[akka://actor-server/system/sharding/GroupPresenceManager#1573901513])
00:54:57.505UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WebrtcCall[0;39m 
                 Coordinator moved from [] to [akka.tcp://actor-server@192.168.1.3:2552]
00:54:57.506UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WebrtcCallCoordinator[0;39m 
                 Singleton manager starting singleton actor [akka://actor-server/system/sharding/WebrtcCallCoordinator/singleton]
00:54:57.506UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WebrtcCallCoordinator[0;39m 
                 ClusterSingletonManager state change [Start -> Oldest]
00:54:57.507UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Get for key [WebrtcCallCoordinatorState], local data [None]
00:54:57.507UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManagerCoordinator/singleton/coordinator[0;39m 
                 ShardRegion registered: [Actor[akka://actor-server/system/sharding/SocialManager#391155454]]
00:54:57.507UTC [34mINFO [0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WebrtcCallCoordinator/singleton/coordinator[0;39m 
                 Sharding Coordinator was moved to the active state State(Map(),Map(),Set(),Set(),false)
00:54:57.507UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [SocialManagerCoordinatorState], old data [None], new data [LWWRegister(State(Map(),Map(Actor[akka://actor-server/system/sharding/SocialManager#391155454] -> Vector()),Set(),Set(),false))]
00:54:57.508UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManagerCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardRegionRegistered(Actor[akka://actor-server/system/sharding/SocialManager#391155454])
00:54:57.538UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessorCoordinator/singleton/coordinator[0;39m 
                 ShardRegion registered: [Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719]]
00:54:57.538UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [UserProcessorCoordinatorState], old data [None], new data [LWWRegister(State(Map(),Map(Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719] -> Vector()),Set(),Set(),false))]
00:54:57.538UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessorCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardRegionRegistered(Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719])
00:54:57.542UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/EventBusMediator[0;39m 
                 Coordinator moved from [] to [akka.tcp://actor-server@192.168.1.3:2552]
00:54:57.544UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/EventBusMediatorCoordinator[0;39m 
                 Singleton manager starting singleton actor [akka://actor-server/system/sharding/EventBusMediatorCoordinator/singleton]
00:54:57.544UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/EventBusMediatorCoordinator[0;39m 
                 ClusterSingletonManager state change [Start -> Oldest]
00:54:57.545UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Get for key [EventBusMediatorCoordinatorState], local data [None]
00:54:57.545UTC [34mINFO [0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/EventBusMediatorCoordinator/singleton/coordinator[0;39m 
                 Sharding Coordinator was moved to the active state State(Map(),Map(),Set(),Set(),false)
00:54:57.546UTC [31mWARN [0;39m [36makka.actor.ActorSystemImpl[0;39m [35makka.actor.ActorSystemImpl(actor-server)[0;39m 
                 Starting BotExtension
00:54:57.551UTC [31mWARN [0;39m [36makka.actor.ActorSystemImpl[0;39m [35makka.actor.ActorSystemImpl(actor-server)[0;39m 
                 Starting ActorBot
00:54:57.565UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/$a[0;39m 
                 Singleton manager starting singleton actor [akka://actor-server/user/$a/singleton]
00:54:57.565UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/$a[0;39m 
                 ClusterSingletonManager state change [Start -> Oldest]
00:54:57.566UTC [31mWARN [0;39m [36mim.actor.server.bot.ActorBot[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/$a/singleton[0;39m 
                 Initiating bot 10 actor Actor Bot
00:54:57.567UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessor[0;39m 
                 Request shard [10] home
00:54:57.567UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [UserProcessorCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(),Map(Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719] -> Vector()),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(10 -> Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719]),Map(Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719] -> Vector(10)),Set(),Set(),false))]
00:54:57.568UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessorCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(10,Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719])
00:54:57.568UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessorCoordinator/singleton/coordinator[0;39m 
                 Shard [10] allocated at [Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719]]
00:54:57.568UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessor[0;39m 
                 Host Shard [10] 
00:54:57.568UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessor[0;39m 
                 Starting shard [10] in region
00:54:57.568UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessor[0;39m 
                 Shard [10] located at [Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719]]
00:54:57.568UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessor[0;39m 
                 Shard was initialized 10
00:54:57.568UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessor[0;39m 
                 Deliver [1] buffered messages for shard [10]
00:54:57.568UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessor/10[0;39m 
                 Starting entity [10] in shard [10]
00:54:57.675UTC [31mWARN [0;39m [36mim.actor.server.bot.ActorBot[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/$a/singleton[0;39m 
                 Bot already exists
00:54:57.840UTC [31mWARN [0;39m [36mim.actor.server.bot.ActorBot[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/$a/singleton[0;39m 
                 Initialized bot 10 actor Actor Bot
00:54:57.957UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/GroupProcessorCoordinator/singleton/coordinator[0;39m 
                 ShardRegion registered: [Actor[akka://actor-server/system/sharding/GroupProcessor#2022796947]]
00:54:57.958UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [GroupProcessorCoordinatorState], old data [None], new data [LWWRegister(State(Map(),Map(Actor[akka://actor-server/system/sharding/GroupProcessor#2022796947] -> Vector()),Set(),Set(),false))]
00:54:57.958UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/GroupProcessorCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardRegionRegistered(Actor[akka://actor-server/system/sharding/GroupProcessor#2022796947])
00:54:58.018UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SessionCoordinator/singleton/coordinator[0;39m 
                 ShardRegion registered: [Actor[akka://actor-server/system/sharding/Session#-2142992107]]
00:54:58.018UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [SessionCoordinatorState], old data [None], new data [LWWRegister(State(Map(),Map(Actor[akka://actor-server/system/sharding/Session#-2142992107] -> Vector()),Set(),Set(),false))]
00:54:58.018UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SessionCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardRegionRegistered(Actor[akka://actor-server/system/sharding/Session#-2142992107])
00:54:58.026UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Request shard [24] home
00:54:58.026UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [WeakUpdatesManagerCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(),Map(Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232] -> Vector()),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(24 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]),Map(Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232] -> Vector(24)),Set(),Set(),false))]
00:54:58.026UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(24,Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232])
00:54:58.026UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 Shard [24] allocated at [Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]]
00:54:58.027UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Host Shard [24] 
00:54:58.027UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Starting shard [24] in region
00:54:58.027UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Shard [24] located at [Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]]
00:54:58.027UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Shard was initialized 24
00:54:58.027UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Deliver [1] buffered messages for shard [24]
00:54:58.028UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager/24[0;39m 
                 Starting entity [3175226625488964280] in shard [24]
00:54:59.518UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WebrtcCallCoordinator/singleton/coordinator[0;39m 
                 ShardRegion registered: [Actor[akka://actor-server/system/sharding/WebrtcCall#-1190604205]]
00:54:59.518UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [WebrtcCallCoordinatorState], old data [None], new data [LWWRegister(State(Map(),Map(Actor[akka://actor-server/system/sharding/WebrtcCall#-1190604205] -> Vector()),Set(),Set(),false))]
00:54:59.518UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WebrtcCallCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardRegionRegistered(Actor[akka://actor-server/system/sharding/WebrtcCall#-1190604205])
00:54:59.557UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/EventBusMediatorCoordinator/singleton/coordinator[0;39m 
                 ShardRegion registered: [Actor[akka://actor-server/system/sharding/EventBusMediator#2079083487]]
00:54:59.558UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [EventBusMediatorCoordinatorState], old data [None], new data [LWWRegister(State(Map(),Map(Actor[akka://actor-server/system/sharding/EventBusMediator#2079083487] -> Vector()),Set(),Set(),false))]
00:54:59.558UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/EventBusMediatorCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardRegionRegistered(Actor[akka://actor-server/system/sharding/EventBusMediator#2079083487])
00:55:21.120UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/Session[0;39m 
                 Request shard [0] home
00:55:21.121UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [SessionCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(),Map(Actor[akka://actor-server/system/sharding/Session#-2142992107] -> Vector()),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(0 -> Actor[akka://actor-server/system/sharding/Session#-2142992107]),Map(Actor[akka://actor-server/system/sharding/Session#-2142992107] -> Vector(0)),Set(),Set(),false))]
00:55:21.121UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SessionCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(0,Actor[akka://actor-server/system/sharding/Session#-2142992107])
00:55:21.121UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SessionCoordinator/singleton/coordinator[0;39m 
                 Shard [0] allocated at [Actor[akka://actor-server/system/sharding/Session#-2142992107]]
00:55:21.121UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/Session[0;39m 
                 Host Shard [0] 
00:55:21.121UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/Session[0;39m 
                 Starting shard [0] in region
00:55:21.122UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/Session[0;39m 
                 Shard [0] located at [Actor[akka://actor-server/system/sharding/Session#-2142992107]]
00:55:21.122UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/Session[0;39m 
                 Shard was initialized 0
00:55:21.122UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/Session[0;39m 
                 Deliver [1] buffered messages for shard [0]
00:55:21.275UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/Session/0[0;39m 
                 Starting entity [-4775611501413901810_-6013645157869679374] in shard [0]
00:56:15.196UTC [1;31mERROR[0;39m [36mim.actor.server.session.RpcHandler[0;39m [35makka://actor-server/system/sharding/Session/0/-4775611501413901810_-6013645157869679374/rpcHandler[0;39m -4775611501413901810
                -6013645157869679374 Failed to process request messageId: 1: Request({ "_name": "RequestStartPhoneAuth","phoneNumber": 556191022123,"appId": 1,"apiKey": "4295f9666fad3faf2d04277fe7a0c40ff39a85d313de5348ad8ffa650ad71855","deviceHash": [B@79374dd6,"deviceTitle": "Unknown Android SDK built for x86","timeZone": "America/Sao_Paulo","preferredLanguages": [ "en-US", "en" ] })
java.util.concurrent.TimeoutException: Future timed out
	at im.actor.concurrent.package$ExtendedFuture$$anonfun$1.apply(package.scala:14) ~[actor-concurrent_2.11-0.0.27-SNAPSHOT.jar:0.0.27-SNAPSHOT]
	at im.actor.concurrent.package$ExtendedFuture$$anonfun$1.apply(package.scala:14) ~[actor-concurrent_2.11-0.0.27-SNAPSHOT.jar:0.0.27-SNAPSHOT]
	at akka.pattern.FutureTimeoutSupport$$anonfun$after$1.liftedTree1$1(FutureTimeoutSupport.scala:25) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.pattern.FutureTimeoutSupport$$anonfun$after$1.apply$mcV$sp(FutureTimeoutSupport.scala:25) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
00:56:15.224UTC [1;31mERROR[0;39m [36mim.actor.server.session.RpcHandler[0;39m [35makka://actor-server/system/sharding/Session/0/-4775611501413901810_-6013645157869679374/rpcHandler[0;39m -4775611501413901810
                -6013645157869679374 Failed to process request messageId: 1: Request({ "_name": "RequestStartPhoneAuth","phoneNumber": 556191022123,"appId": 1,"apiKey": "4295f9666fad3faf2d04277fe7a0c40ff39a85d313de5348ad8ffa650ad71855","deviceHash": [B@79374dd6,"deviceTitle": "Unknown Android SDK built for x86","timeZone": "America/Sao_Paulo","preferredLanguages": [ "en-US", "en" ] })
java.util.concurrent.TimeoutException: Future timed out
	at im.actor.concurrent.package$ExtendedFuture$$anonfun$1.apply(package.scala:14) ~[actor-concurrent_2.11-0.0.27-SNAPSHOT.jar:0.0.27-SNAPSHOT]
	at im.actor.concurrent.package$ExtendedFuture$$anonfun$1.apply(package.scala:14) ~[actor-concurrent_2.11-0.0.27-SNAPSHOT.jar:0.0.27-SNAPSHOT]
	at akka.pattern.FutureTimeoutSupport$$anonfun$after$1.liftedTree1$1(FutureTimeoutSupport.scala:25) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.pattern.FutureTimeoutSupport$$anonfun$after$1.apply$mcV$sp(FutureTimeoutSupport.scala:25) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
00:56:15.225UTC [1;31mERROR[0;39m [36mim.actor.server.session.RpcHandler[0;39m [35makka://actor-server/system/sharding/Session/0/-4775611501413901810_-6013645157869679374/rpcHandler[0;39m -4775611501413901810
                -6013645157869679374 Failed to process request messageId: 3: Request({ "_name": "RequestRegisterGooglePush","projectId": 43880936595,"token": "f7BGZuo3n-8:APA91bH4sdf65fBOi1zQuFoGBTbnbXjxkf3BvcgHtXE6f0CB3o6DqlO6CAwTBE-p7UHkHASsvBvTkM1QbHHrMKOfGwpI_Y5e16YelS9DOZwwZe6JZdbLrnH_tDGP1wcc0iYpqv0Ex1Y-" })
org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "google_push_credentials_pkey"
  Detail: Key (auth_id)=(-4775611501413901810) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2284) ~[postgresql-9.4.1208.jar:9.4.1208]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2003) ~[postgresql-9.4.1208.jar:9.4.1208]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:200) ~[postgresql-9.4.1208.jar:9.4.1208]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:424) ~[postgresql-9.4.1208.jar:9.4.1208]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:161) ~[postgresql-9.4.1208.jar:9.4.1208]
	at org.postgresql.jdbc.PgPreparedStatement.executeUpdate(PgPreparedStatement.java:133) ~[postgresql-9.4.1208.jar:9.4.1208]
	at com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61) ~[HikariCP-2.4.6.jar:na]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeUpdate(HikariProxyPreparedStatement.java) ~[HikariCP-2.4.6.jar:na]
	at slick.driver.JdbcActionComponent$InsertActionComposerImpl$InsertOrUpdateAction$$anonfun$nativeUpsert$1.apply(JdbcActionComponent.scala:560) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.driver.JdbcActionComponent$InsertActionComposerImpl$InsertOrUpdateAction$$anonfun$nativeUpsert$1.apply(JdbcActionComponent.scala:557) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.jdbc.JdbcBackend$SessionDef$class.withPreparedStatement(JdbcBackend.scala:349) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:409) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.driver.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:498) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.driver.JdbcActionComponent$InsertActionComposerImpl$InsertOrUpdateAction.nativeUpsert(JdbcActionComponent.scala:557) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.driver.JdbcActionComponent$InsertActionComposerImpl$InsertOrUpdateAction.f$1(JdbcActionComponent.scala:540) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.driver.JdbcActionComponent$InsertActionComposerImpl$InsertOrUpdateAction.run(JdbcActionComponent.scala:545) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.driver.JdbcActionComponent$SimpleJdbcDriverAction.run(JdbcActionComponent.scala:32) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.driver.JdbcActionComponent$SimpleJdbcDriverAction.run(JdbcActionComponent.scala:29) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:240) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:240) ~[slick_2.11-3.1.1.2.jar:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_131]
	at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_131]
00:56:15.227UTC [1;31mERROR[0;39m [36mim.actor.server.session.RpcHandler[0;39m [35makka://actor-server/system/sharding/Session/0/-4775611501413901810_-6013645157869679374/rpcHandler[0;39m -4775611501413901810
                -6013645157869679374 Failed to process request messageId: 3: Request({ "_name": "RequestRegisterGooglePush","projectId": 43880936595,"token": "f7BGZuo3n-8:APA91bH4sdf65fBOi1zQuFoGBTbnbXjxkf3BvcgHtXE6f0CB3o6DqlO6CAwTBE-p7UHkHASsvBvTkM1QbHHrMKOfGwpI_Y5e16YelS9DOZwwZe6JZdbLrnH_tDGP1wcc0iYpqv0Ex1Y-" })
org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "google_push_credentials_pkey"
  Detail: Key (auth_id)=(-4775611501413901810) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2284) ~[postgresql-9.4.1208.jar:9.4.1208]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2003) ~[postgresql-9.4.1208.jar:9.4.1208]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:200) ~[postgresql-9.4.1208.jar:9.4.1208]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:424) ~[postgresql-9.4.1208.jar:9.4.1208]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:161) ~[postgresql-9.4.1208.jar:9.4.1208]
	at org.postgresql.jdbc.PgPreparedStatement.executeUpdate(PgPreparedStatement.java:133) ~[postgresql-9.4.1208.jar:9.4.1208]
	at com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61) ~[HikariCP-2.4.6.jar:na]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeUpdate(HikariProxyPreparedStatement.java) ~[HikariCP-2.4.6.jar:na]
	at slick.driver.JdbcActionComponent$InsertActionComposerImpl$InsertOrUpdateAction$$anonfun$nativeUpsert$1.apply(JdbcActionComponent.scala:560) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.driver.JdbcActionComponent$InsertActionComposerImpl$InsertOrUpdateAction$$anonfun$nativeUpsert$1.apply(JdbcActionComponent.scala:557) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.jdbc.JdbcBackend$SessionDef$class.withPreparedStatement(JdbcBackend.scala:349) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:409) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.driver.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:498) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.driver.JdbcActionComponent$InsertActionComposerImpl$InsertOrUpdateAction.nativeUpsert(JdbcActionComponent.scala:557) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.driver.JdbcActionComponent$InsertActionComposerImpl$InsertOrUpdateAction.f$1(JdbcActionComponent.scala:540) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.driver.JdbcActionComponent$InsertActionComposerImpl$InsertOrUpdateAction.run(JdbcActionComponent.scala:545) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.driver.JdbcActionComponent$SimpleJdbcDriverAction.run(JdbcActionComponent.scala:32) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.driver.JdbcActionComponent$SimpleJdbcDriverAction.run(JdbcActionComponent.scala:29) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:240) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:240) ~[slick_2.11-3.1.1.2.jar:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_131]
	at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_131]
00:56:15.232UTC [1;31mERROR[0;39m [36mi.a.s.sequence.SeqUpdatesExtension[0;39m [35mSeqUpdatesExtension(akka://actor-server)[0;39m 
                 AuthSession not found
java.lang.RuntimeException: AuthSession not found
	at im.actor.server.sequence.operations.PushOperations$class.im$actor$server$sequence$operations$PushOperations$class$$$anonfun$14(PushOperations.scala:100) ~[classes/:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
00:56:15.233UTC [1;31mERROR[0;39m [36mim.actor.server.session.RpcHandler[0;39m [35makka://actor-server/system/sharding/Session/0/-4775611501413901810_-6013645157869679374/rpcHandler[0;39m -4775611501413901810
                -6013645157869679374 Failed to process request messageId: 2: Request({ "_name": "RequestRegisterGooglePush","projectId": 43880936595,"token": "f7BGZuo3n-8:APA91bH4sdf65fBOi1zQuFoGBTbnbXjxkf3BvcgHtXE6f0CB3o6DqlO6CAwTBE-p7UHkHASsvBvTkM1QbHHrMKOfGwpI_Y5e16YelS9DOZwwZe6JZdbLrnH_tDGP1wcc0iYpqv0Ex1Y-" })
java.lang.RuntimeException: AuthSession not found
	at im.actor.server.sequence.operations.PushOperations$class.im$actor$server$sequence$operations$PushOperations$class$$$anonfun$14(PushOperations.scala:100) ~[classes/:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
00:56:15.235UTC [1;31mERROR[0;39m [36mim.actor.server.session.RpcHandler[0;39m [35makka://actor-server/system/sharding/Session/0/-4775611501413901810_-6013645157869679374/rpcHandler[0;39m -4775611501413901810
                -6013645157869679374 Failed to process request messageId: 2: Request({ "_name": "RequestRegisterGooglePush","projectId": 43880936595,"token": "f7BGZuo3n-8:APA91bH4sdf65fBOi1zQuFoGBTbnbXjxkf3BvcgHtXE6f0CB3o6DqlO6CAwTBE-p7UHkHASsvBvTkM1QbHHrMKOfGwpI_Y5e16YelS9DOZwwZe6JZdbLrnH_tDGP1wcc0iYpqv0Ex1Y-" })
java.lang.RuntimeException: AuthSession not found
	at im.actor.server.sequence.operations.PushOperations$class.im$actor$server$sequence$operations$PushOperations$class$$$anonfun$14(PushOperations.scala:100) ~[classes/:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
00:56:41.701UTC [1;31mERROR[0;39m [36mi.a.s.sequence.SeqUpdatesExtension[0;39m [35mSeqUpdatesExtension(akka://actor-server)[0;39m 
                 AuthSession not found
java.lang.RuntimeException: AuthSession not found
	at im.actor.server.sequence.operations.PushOperations$class.im$actor$server$sequence$operations$PushOperations$class$$$anonfun$14(PushOperations.scala:100) ~[classes/:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
00:56:43.158UTC [1;31mERROR[0;39m [36mi.a.s.sequence.SeqUpdatesExtension[0;39m [35mSeqUpdatesExtension(akka://actor-server)[0;39m 
                 AuthSession not found
java.lang.RuntimeException: AuthSession not found
	at im.actor.server.sequence.operations.PushOperations$class.im$actor$server$sequence$operations$PushOperations$class$$$anonfun$14(PushOperations.scala:100) ~[classes/:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
00:56:43.587UTC [1;31mERROR[0;39m [36mi.a.s.sequence.SeqUpdatesExtension[0;39m [35mSeqUpdatesExtension(akka://actor-server)[0;39m 
                 AuthSession not found
java.lang.RuntimeException: AuthSession not found
	at im.actor.server.sequence.operations.PushOperations$class.im$actor$server$sequence$operations$PushOperations$class$$$anonfun$14(PushOperations.scala:100) ~[classes/:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
00:56:43.597UTC [1;31mERROR[0;39m [36mi.a.s.sequence.SeqUpdatesExtension[0;39m [35mSeqUpdatesExtension(akka://actor-server)[0;39m 
                 AuthSession not found
java.lang.RuntimeException: AuthSession not found
	at im.actor.server.sequence.operations.PushOperations$class.im$actor$server$sequence$operations$PushOperations$class$$$anonfun$14(PushOperations.scala:100) ~[classes/:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
00:56:43.618UTC [1;31mERROR[0;39m [36mim.actor.server.session.RpcHandler[0;39m [35makka://actor-server/system/sharding/Session/0/-4775611501413901810_-6013645157869679374/rpcHandler[0;39m -4775611501413901810
                -6013645157869679374 Failed to process request messageId: 11: Request({ "_name": "RequestRegisterGooglePush","projectId": 43880936595,"token": "f7BGZuo3n-8:APA91bH4sdf65fBOi1zQuFoGBTbnbXjxkf3BvcgHtXE6f0CB3o6DqlO6CAwTBE-p7UHkHASsvBvTkM1QbHHrMKOfGwpI_Y5e16YelS9DOZwwZe6JZdbLrnH_tDGP1wcc0iYpqv0Ex1Y-" })
java.lang.RuntimeException: AuthSession not found
	at im.actor.server.sequence.operations.PushOperations$class.im$actor$server$sequence$operations$PushOperations$class$$$anonfun$14(PushOperations.scala:100) ~[classes/:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
00:56:43.624UTC [1;31mERROR[0;39m [36mim.actor.server.session.RpcHandler[0;39m [35makka://actor-server/system/sharding/Session/0/-4775611501413901810_-6013645157869679374/rpcHandler[0;39m -4775611501413901810
                -6013645157869679374 Failed to process request messageId: 12: Request({ "_name": "RequestRegisterGooglePush","projectId": 43880936595,"token": "f7BGZuo3n-8:APA91bH4sdf65fBOi1zQuFoGBTbnbXjxkf3BvcgHtXE6f0CB3o6DqlO6CAwTBE-p7UHkHASsvBvTkM1QbHHrMKOfGwpI_Y5e16YelS9DOZwwZe6JZdbLrnH_tDGP1wcc0iYpqv0Ex1Y-" })
java.lang.RuntimeException: AuthSession not found
	at im.actor.server.sequence.operations.PushOperations$class.im$actor$server$sequence$operations$PushOperations$class$$$anonfun$14(PushOperations.scala:100) ~[classes/:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
00:56:43.626UTC [1;31mERROR[0;39m [36mim.actor.server.session.RpcHandler[0;39m [35makka://actor-server/system/sharding/Session/0/-4775611501413901810_-6013645157869679374/rpcHandler[0;39m -4775611501413901810
                -6013645157869679374 Failed to process request messageId: 15: Request({ "_name": "RequestRegisterGooglePush","projectId": 43880936595,"token": "f7BGZuo3n-8:APA91bH4sdf65fBOi1zQuFoGBTbnbXjxkf3BvcgHtXE6f0CB3o6DqlO6CAwTBE-p7UHkHASsvBvTkM1QbHHrMKOfGwpI_Y5e16YelS9DOZwwZe6JZdbLrnH_tDGP1wcc0iYpqv0Ex1Y-" })
java.lang.RuntimeException: AuthSession not found
	at im.actor.server.sequence.operations.PushOperations$class.im$actor$server$sequence$operations$PushOperations$class$$$anonfun$14(PushOperations.scala:100) ~[classes/:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
00:56:43.626UTC [1;31mERROR[0;39m [36mim.actor.server.session.RpcHandler[0;39m [35makka://actor-server/system/sharding/Session/0/-4775611501413901810_-6013645157869679374/rpcHandler[0;39m -4775611501413901810
                -6013645157869679374 Failed to process request messageId: 16: Request({ "_name": "RequestRegisterGooglePush","projectId": 43880936595,"token": "f7BGZuo3n-8:APA91bH4sdf65fBOi1zQuFoGBTbnbXjxkf3BvcgHtXE6f0CB3o6DqlO6CAwTBE-p7UHkHASsvBvTkM1QbHHrMKOfGwpI_Y5e16YelS9DOZwwZe6JZdbLrnH_tDGP1wcc0iYpqv0Ex1Y-" })
java.lang.RuntimeException: AuthSession not found
	at im.actor.server.sequence.operations.PushOperations$class.im$actor$server$sequence$operations$PushOperations$class$$$anonfun$14(PushOperations.scala:100) ~[classes/:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
00:56:43.627UTC [1;31mERROR[0;39m [36mim.actor.server.session.RpcHandler[0;39m [35makka://actor-server/system/sharding/Session/0/-4775611501413901810_-6013645157869679374/rpcHandler[0;39m -4775611501413901810
                -6013645157869679374 Failed to process request messageId: 12: Request({ "_name": "RequestRegisterGooglePush","projectId": 43880936595,"token": "f7BGZuo3n-8:APA91bH4sdf65fBOi1zQuFoGBTbnbXjxkf3BvcgHtXE6f0CB3o6DqlO6CAwTBE-p7UHkHASsvBvTkM1QbHHrMKOfGwpI_Y5e16YelS9DOZwwZe6JZdbLrnH_tDGP1wcc0iYpqv0Ex1Y-" })
java.lang.RuntimeException: AuthSession not found
	at im.actor.server.sequence.operations.PushOperations$class.im$actor$server$sequence$operations$PushOperations$class$$$anonfun$14(PushOperations.scala:100) ~[classes/:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
00:56:43.627UTC [1;31mERROR[0;39m [36mim.actor.server.session.RpcHandler[0;39m [35makka://actor-server/system/sharding/Session/0/-4775611501413901810_-6013645157869679374/rpcHandler[0;39m -4775611501413901810
                -6013645157869679374 Failed to process request messageId: 16: Request({ "_name": "RequestRegisterGooglePush","projectId": 43880936595,"token": "f7BGZuo3n-8:APA91bH4sdf65fBOi1zQuFoGBTbnbXjxkf3BvcgHtXE6f0CB3o6DqlO6CAwTBE-p7UHkHASsvBvTkM1QbHHrMKOfGwpI_Y5e16YelS9DOZwwZe6JZdbLrnH_tDGP1wcc0iYpqv0Ex1Y-" })
java.lang.RuntimeException: AuthSession not found
	at im.actor.server.sequence.operations.PushOperations$class.im$actor$server$sequence$operations$PushOperations$class$$$anonfun$14(PushOperations.scala:100) ~[classes/:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
00:56:43.689UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessor[0;39m 
                 Request shard [48] home
00:56:43.689UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [UserProcessorCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(10 -> Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719]),Map(Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719] -> Vector(10)),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(10 -> Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719], 48 -> Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719]),Map(Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719] -> Vector(10, 48)),Set(),Set(),false))]
00:56:43.689UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessorCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(48,Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719])
00:56:43.690UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessorCoordinator/singleton/coordinator[0;39m 
                 Shard [48] allocated at [Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719]]
00:56:43.690UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessor[0;39m 
                 Host Shard [48] 
00:56:43.690UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessor[0;39m 
                 Starting shard [48] in region
00:56:43.690UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessor[0;39m 
                 Shard [48] located at [Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719]]
00:56:43.690UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessor[0;39m 
                 Shard was initialized 48
00:56:43.690UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessor[0;39m 
                 Deliver [1] buffered messages for shard [48]
00:56:43.690UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessor/48[0;39m 
                 Starting entity [1955255348] in shard [48]
00:56:43.886UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Request shard [-18] home
00:56:43.886UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManagerCoordinator[0;39m 
                 Singleton manager starting singleton actor [akka://actor-server/system/sharding/SeqUpdatesManagerCoordinator/singleton]
00:56:43.886UTC [34mINFO [0;39m [36ma.c.s.ClusterSingletonManager[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManagerCoordinator[0;39m 
                 ClusterSingletonManager state change [Start -> Oldest]
00:56:43.886UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [WeakUpdatesManagerCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(24 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]),Map(Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232] -> Vector(24)),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(24 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -18 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]),Map(Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232] -> Vector(24, -18)),Set(),Set(),false))]
00:56:43.886UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(-18,Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232])
00:56:43.886UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 Shard [-18] allocated at [Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]]
00:56:43.886UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Host Shard [-18] 
00:56:43.886UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Starting shard [-18] in region
00:56:43.887UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Shard [-18] located at [Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]]
00:56:43.887UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Shard was initialized -18
00:56:43.887UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Deliver [1] buffered messages for shard [-18]
00:56:43.887UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager/-18[0;39m 
                 Starting entity [-4775611501413901810] in shard [-18]
00:56:43.887UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManager[0;39m 
                 Coordinator moved from [] to [akka.tcp://actor-server@192.168.1.3:2552]
00:56:43.887UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Get for key [SeqUpdatesManagerCoordinatorState], local data [None]
00:56:43.893UTC [34mINFO [0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 Sharding Coordinator was moved to the active state State(Map(),Map(),Set(),Set(),false)
00:56:43.893UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManager[0;39m 
                 Request shard [8] home
00:56:44.230UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManager[0;39m 
                 Request shard [20] home
00:56:44.231UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [PresenceManagerCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(),Map(Actor[akka://actor-server/system/sharding/PresenceManager#-2053734734] -> Vector()),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(20 -> Actor[akka://actor-server/system/sharding/PresenceManager#-2053734734]),Map(Actor[akka://actor-server/system/sharding/PresenceManager#-2053734734] -> Vector(20)),Set(),Set(),false))]
00:56:44.231UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManagerCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(20,Actor[akka://actor-server/system/sharding/PresenceManager#-2053734734])
00:56:44.231UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManagerCoordinator/singleton/coordinator[0;39m 
                 Shard [20] allocated at [Actor[akka://actor-server/system/sharding/PresenceManager#-2053734734]]
00:56:44.231UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManager[0;39m 
                 Host Shard [20] 
00:56:44.231UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManager[0;39m 
                 Starting shard [20] in region
00:56:44.231UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManager[0;39m 
                 Shard [20] located at [Actor[akka://actor-server/system/sharding/PresenceManager#-2053734734]]
00:56:44.231UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManager[0;39m 
                 Shard was initialized 20
00:56:44.231UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManager[0;39m 
                 Deliver [1] buffered messages for shard [20]
00:56:44.232UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManager/20[0;39m 
                 Starting entity [1955255348] in shard [20]
00:56:44.633UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessor[0;39m 
                 Request shard [6] home
00:56:44.634UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [UserProcessorCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(10 -> Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719], 48 -> Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719]),Map(Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719] -> Vector(10, 48)),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(10 -> Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719], 48 -> Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719], 6 -> Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719]),Map(Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719] -> Vector(10, 48, 6)),Set(),Set(),false))]
00:56:44.634UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessorCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(6,Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719])
00:56:44.634UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessorCoordinator/singleton/coordinator[0;39m 
                 Shard [6] allocated at [Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719]]
00:56:44.634UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessor[0;39m 
                 Host Shard [6] 
00:56:44.634UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessor[0;39m 
                 Starting shard [6] in region
00:56:44.634UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessor[0;39m 
                 Shard [6] located at [Actor[akka://actor-server/system/sharding/UserProcessor#-1964594719]]
00:56:44.634UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessor[0;39m 
                 Shard was initialized 6
00:56:44.634UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessor[0;39m 
                 Deliver [1] buffered messages for shard [6]
00:56:44.634UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessor/6[0;39m 
                 Starting entity [266296106] in shard [6]
00:56:45.897UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 ShardRegion registered: [Actor[akka://actor-server/system/sharding/SeqUpdatesManager#-360359801]]
00:56:45.898UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [SeqUpdatesManagerCoordinatorState], old data [None], new data [LWWRegister(State(Map(),Map(Actor[akka://actor-server/system/sharding/SeqUpdatesManager#-360359801] -> Vector()),Set(),Set(),false))]
00:56:45.898UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardRegionRegistered(Actor[akka://actor-server/system/sharding/SeqUpdatesManager#-360359801])
00:56:45.898UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManager[0;39m 
                 Retry request for shard [8] homes from coordinator at [Actor[akka://actor-server/system/sharding/SeqUpdatesManagerCoordinator/singleton/coordinator#-1186085645]]. [4] buffered messages.
00:56:45.898UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [SeqUpdatesManagerCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(),Map(Actor[akka://actor-server/system/sharding/SeqUpdatesManager#-360359801] -> Vector()),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(8 -> Actor[akka://actor-server/system/sharding/SeqUpdatesManager#-360359801]),Map(Actor[akka://actor-server/system/sharding/SeqUpdatesManager#-360359801] -> Vector(8)),Set(),Set(),false))]
00:56:45.898UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(8,Actor[akka://actor-server/system/sharding/SeqUpdatesManager#-360359801])
00:56:45.898UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 Shard [8] allocated at [Actor[akka://actor-server/system/sharding/SeqUpdatesManager#-360359801]]
00:56:45.898UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManager[0;39m 
                 Host Shard [8] 
00:56:45.898UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManager[0;39m 
                 Starting shard [8] in region
00:56:45.899UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManager[0;39m 
                 Shard [8] located at [Actor[akka://actor-server/system/sharding/SeqUpdatesManager#-360359801]]
00:56:45.899UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManager[0;39m 
                 Shard was initialized 8
00:56:45.899UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManager[0;39m 
                 Deliver [4] buffered messages for shard [8]
00:56:45.902UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManager/8[0;39m 
                 Starting entity [1955255348] in shard [8]
00:57:00.534UTC [39mDEBUG[0;39m [36mi.a.s.push.google.DeliveryStream[0;39m [35mDeliveryStream(akka://actor-server)[0;39m 
                 Starting GCM stream
00:57:00.569UTC [39mDEBUG[0;39m [36mi.a.s.push.google.DeliveryStream[0;39m [35mDeliveryStream(akka://actor-server)[0;39m 
                 Starting Firebase stream
00:57:00.613UTC [39mDEBUG[0;39m [36mi.a.s.push.google.GooglePushDelivery[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/fcm-delivery[0;39m 
                 Trying to deliver google push. Queue size: 0, totalDemand: 16, subscriber requests 16 elements
00:57:00.613UTC [39mDEBUG[0;39m [36mi.a.s.push.google.GooglePushDelivery[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/gcm-delivery[0;39m 
                 Trying to deliver google push. Queue size: 0, totalDemand: 16, subscriber requests 16 elements
00:57:02.791UTC [39mDEBUG[0;39m [36mi.a.s.push.apple.ApplePushExtension[0;39m [35mim.actor.server.push.apple.ApplePushExtension[0;39m 
                 Established client connection for cert: 868547, is voip: true
00:57:02.791UTC [39mDEBUG[0;39m [36mi.a.s.push.apple.ApplePushExtension[0;39m [35mim.actor.server.push.apple.ApplePushExtension[0;39m 
                 Established client connection for cert: 868547, is voip: false
00:59:21.840UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManager[0;39m 
                 Request shard [10] home
00:59:21.841UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [PresenceManagerCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(20 -> Actor[akka://actor-server/system/sharding/PresenceManager#-2053734734]),Map(Actor[akka://actor-server/system/sharding/PresenceManager#-2053734734] -> Vector(20)),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(20 -> Actor[akka://actor-server/system/sharding/PresenceManager#-2053734734], 10 -> Actor[akka://actor-server/system/sharding/PresenceManager#-2053734734]),Map(Actor[akka://actor-server/system/sharding/PresenceManager#-2053734734] -> Vector(20, 10)),Set(),Set(),false))]
00:59:21.842UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManagerCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(10,Actor[akka://actor-server/system/sharding/PresenceManager#-2053734734])
00:59:21.842UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManagerCoordinator/singleton/coordinator[0;39m 
                 Shard [10] allocated at [Actor[akka://actor-server/system/sharding/PresenceManager#-2053734734]]
00:59:21.842UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManager[0;39m 
                 Host Shard [10] 
00:59:21.842UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManager[0;39m 
                 Starting shard [10] in region
00:59:21.843UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManager[0;39m 
                 Shard was initialized 10
00:59:21.843UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManager[0;39m 
                 Deliver [1] buffered messages for shard [10]
00:59:21.843UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManager/10[0;39m 
                 Starting entity [266296106] in shard [10]
00:59:21.845UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/PresenceManager[0;39m 
                 Shard [10] located at [Actor[akka://actor-server/system/sharding/PresenceManager#-2053734734]]
00:59:21.978UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManager[0;39m 
                 Request shard [6] home
00:59:21.978UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [SeqUpdatesManagerCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(8 -> Actor[akka://actor-server/system/sharding/SeqUpdatesManager#-360359801]),Map(Actor[akka://actor-server/system/sharding/SeqUpdatesManager#-360359801] -> Vector(8)),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(8 -> Actor[akka://actor-server/system/sharding/SeqUpdatesManager#-360359801], 6 -> Actor[akka://actor-server/system/sharding/SeqUpdatesManager#-360359801]),Map(Actor[akka://actor-server/system/sharding/SeqUpdatesManager#-360359801] -> Vector(8, 6)),Set(),Set(),false))]
00:59:21.978UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(6,Actor[akka://actor-server/system/sharding/SeqUpdatesManager#-360359801])
00:59:21.978UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 Shard [6] allocated at [Actor[akka://actor-server/system/sharding/SeqUpdatesManager#-360359801]]
00:59:21.978UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManager[0;39m 
                 Host Shard [6] 
00:59:21.978UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManager[0;39m 
                 Starting shard [6] in region
00:59:21.979UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManager[0;39m 
                 Shard [6] located at [Actor[akka://actor-server/system/sharding/SeqUpdatesManager#-360359801]]
00:59:21.979UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManager[0;39m 
                 Shard was initialized 6
00:59:21.979UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManager[0;39m 
                 Deliver [1] buffered messages for shard [6]
00:59:21.979UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SeqUpdatesManager/6[0;39m 
                 Starting entity [266296106] in shard [6]
00:59:22.111UTC [39mDEBUG[0;39m [36mi.a.s.push.google.GooglePushDelivery[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/gcm-delivery[0;39m 
                 Trying to deliver google push. Queue size: 0, totalDemand: 16
00:59:28.802UTC [39mDEBUG[0;39m [36mi.a.s.push.google.DeliveryStream[0;39m [35mDeliveryStream(akka://actor-server)[0;39m 
                 GCM: Successfully delivered: Delivery(GooglePushMessage(f7BGZuo3n-8:APA91bH4sdf65fBOi1zQuFoGBTbnbXjxkf3BvcgHtXE6f0CB3o6DqlO6CAwTBE-p7UHkHASsvBvTkM1QbHHrMKOfGwpI_Y5e16YelS9DOZwwZe6JZdbLrnH_tDGP1wcc0iYpqv0Ex1Y-,Some(seq-invisible-1955255348),Some(Map(seq -> 2, _authId -> -4775611501413901810)),None),AIzaSyAliS4bfKy8GucSRX1pnCLJHM5HroHPpLk)
00:59:55.147UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations/53[0;39m 
                 Entity stopped [2015-08-04-UsersMigration]
00:59:55.177UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations/74[0;39m 
                 Entity stopped [2015-08-04-GroupsMigration]
00:59:55.187UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations/67[0;39m 
                 Entity stopped [2015-08-20-LocalNamesMigration]
00:59:55.208UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations/24[0;39m 
                 Entity stopped [2015-08-29-GroupCreatorMemberMigration]
00:59:55.228UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations/63[0;39m 
                 Entity stopped [PutHiddenGroupsToSQL]
00:59:55.237UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations/93[0;39m 
                 Entity stopped [2015-11-03-LocalNamesFromKVMigration]
00:59:55.258UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations/94[0;39m 
                 Entity stopped [2015-11-11-FillUserSequence]
00:59:55.268UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations/76[0;39m 
                 Entity stopped [2015-11-12-FixUserSequence]
00:59:55.967UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/shardakka-kv-Migrations/1[0;39m 
                 Entity stopped [2015-08-21-IntegrationTokenMigration]
01:03:46.553UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/Session/0[0;39m 
                 Starting entity [-4775611501413901810_6696643800307466132] in shard [0]
 [31mWARN [0;39m [36mcom.zaxxer.hikari.pool.HikariPool[0;39m [35m[0;39m 
                 1m764ms844μs711ns - Thread starvation or clock leap detected (housekeeper delta=).
01:04:42.349UTC [1;31mERROR[0;39m [36mim.actor.server.session.RpcHandler[0;39m [35makka://actor-server/system/sharding/Session/0/-4775611501413901810_6696643800307466132/rpcHandler[0;39m -4775611501413901810
                6696643800307466132 Failed to process request messageId: 3: Request({ "_name": "RequestGetDifference","seq": 2,"state": [B@730b77ad,"optimizations": [ 2, 5, 6, 7 ] })
java.util.concurrent.TimeoutException: Future timed out
	at im.actor.concurrent.package$ExtendedFuture$$anonfun$1.apply(package.scala:14) ~[actor-concurrent_2.11-0.0.27-SNAPSHOT.jar:0.0.27-SNAPSHOT]
	at im.actor.concurrent.package$ExtendedFuture$$anonfun$1.apply(package.scala:14) ~[actor-concurrent_2.11-0.0.27-SNAPSHOT.jar:0.0.27-SNAPSHOT]
	at akka.pattern.FutureTimeoutSupport$$anonfun$after$1.liftedTree1$1(FutureTimeoutSupport.scala:25) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.pattern.FutureTimeoutSupport$$anonfun$after$1.apply$mcV$sp(FutureTimeoutSupport.scala:25) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
01:04:42.351UTC [1;31mERROR[0;39m [36mim.actor.server.session.RpcHandler[0;39m [35makka://actor-server/system/sharding/Session/0/-4775611501413901810_6696643800307466132/rpcHandler[0;39m -4775611501413901810
                6696643800307466132 Failed to process request messageId: 3: Request({ "_name": "RequestGetDifference","seq": 2,"state": [B@730b77ad,"optimizations": [ 2, 5, 6, 7 ] })
java.util.concurrent.TimeoutException: Future timed out
	at im.actor.concurrent.package$ExtendedFuture$$anonfun$1.apply(package.scala:14) ~[actor-concurrent_2.11-0.0.27-SNAPSHOT.jar:0.0.27-SNAPSHOT]
	at im.actor.concurrent.package$ExtendedFuture$$anonfun$1.apply(package.scala:14) ~[actor-concurrent_2.11-0.0.27-SNAPSHOT.jar:0.0.27-SNAPSHOT]
	at akka.pattern.FutureTimeoutSupport$$anonfun$after$1.liftedTree1$1(FutureTimeoutSupport.scala:25) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.pattern.FutureTimeoutSupport$$anonfun$after$1.apply$mcV$sp(FutureTimeoutSupport.scala:25) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
01:04:44.693UTC [39mDEBUG[0;39m [36mi.a.s.push.google.GooglePushDelivery[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/gcm-delivery[0;39m 
                 Trying to deliver google push. Queue size: 0, totalDemand: 15
01:04:44.710UTC [31mWARN [0;39m [36mim.actor.server.dialog.DialogRoot[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/UserProcessor/48/1955255348/DialogRoot[0;39m 
                 Unhandled message of class im.actor.server.sequence.SeqState: seq: 1
state: "\020\214\001"

01:04:45.058UTC [39mDEBUG[0;39m [36mi.a.s.push.google.DeliveryStream[0;39m [35mDeliveryStream(akka://actor-server)[0;39m 
                 GCM: Successfully delivered: Delivery(GooglePushMessage(f7BGZuo3n-8:APA91bH4sdf65fBOi1zQuFoGBTbnbXjxkf3BvcgHtXE6f0CB3o6DqlO6CAwTBE-p7UHkHASsvBvTkM1QbHHrMKOfGwpI_Y5e16YelS9DOZwwZe6JZdbLrnH_tDGP1wcc0iYpqv0Ex1Y-,Some(seq-invisible-1955255348),Some(Map(seq -> 3, _authId -> -4775611501413901810)),None),AIzaSyAliS4bfKy8GucSRX1pnCLJHM5HroHPpLk)
01:05:40.922UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/Session/0[0;39m 
                 Starting entity [-4775611501413901810_-4254901328338094540] in shard [0]
 [31mWARN [0;39m [36mcom.zaxxer.hikari.pool.HikariPool[0;39m [35m[0;39m 
                 47s938ms414μs541ns - Thread starvation or clock leap detected (housekeeper delta=).
01:07:38.541UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/Session[0;39m 
                 Request shard [2] home
01:07:38.541UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [SessionCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(0 -> Actor[akka://actor-server/system/sharding/Session#-2142992107]),Map(Actor[akka://actor-server/system/sharding/Session#-2142992107] -> Vector(0)),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(0 -> Actor[akka://actor-server/system/sharding/Session#-2142992107], 2 -> Actor[akka://actor-server/system/sharding/Session#-2142992107]),Map(Actor[akka://actor-server/system/sharding/Session#-2142992107] -> Vector(0, 2)),Set(),Set(),false))]
01:07:38.541UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SessionCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(2,Actor[akka://actor-server/system/sharding/Session#-2142992107])
01:07:38.541UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SessionCoordinator/singleton/coordinator[0;39m 
                 Shard [2] allocated at [Actor[akka://actor-server/system/sharding/Session#-2142992107]]
01:07:38.541UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/Session[0;39m 
                 Host Shard [2] 
01:07:38.541UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/Session[0;39m 
                 Starting shard [2] in region
01:07:38.541UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/Session[0;39m 
                 Shard [2] located at [Actor[akka://actor-server/system/sharding/Session#-2142992107]]
01:07:38.541UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/Session[0;39m 
                 Shard was initialized 2
01:07:38.541UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/Session[0;39m 
                 Deliver [1] buffered messages for shard [2]
01:07:38.542UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/Session/2[0;39m 
                 Starting entity [6748421325664610582_-8448704640517614576] in shard [2]
 [31mWARN [0;39m [36mcom.zaxxer.hikari.pool.HikariPool[0;39m [35m[0;39m 
                 1m17s189ms940μs779ns - Thread starvation or clock leap detected (housekeeper delta=).
01:11:07.343UTC [1;31mERROR[0;39m [36mim.actor.server.session.RpcHandler[0;39m [35makka://actor-server/system/sharding/Session/2/6748421325664610582_-8448704640517614576/rpcHandler[0;39m 6748421325664610582
                -8448704640517614576 Failed to process request messageId: 4: Request({ "_name": "RequestStartPhoneAuth","phoneNumber": 556181122299,"appId": 1,"apiKey": "4295f9666fad3faf2d04277fe7a0c40ff39a85d313de5348ad8ffa650ad71855","deviceHash": [B@63d9eec0,"deviceTitle": "Xiaomi MI 6","timeZone": "America/Sao_Paulo","preferredLanguages": [ "en-GB", "en" ] })
java.util.concurrent.TimeoutException: Future timed out
	at im.actor.concurrent.package$ExtendedFuture$$anonfun$1.apply(package.scala:14) ~[actor-concurrent_2.11-0.0.27-SNAPSHOT.jar:0.0.27-SNAPSHOT]
	at im.actor.concurrent.package$ExtendedFuture$$anonfun$1.apply(package.scala:14) ~[actor-concurrent_2.11-0.0.27-SNAPSHOT.jar:0.0.27-SNAPSHOT]
	at akka.pattern.FutureTimeoutSupport$$anonfun$after$1.liftedTree1$1(FutureTimeoutSupport.scala:25) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.pattern.FutureTimeoutSupport$$anonfun$after$1.apply$mcV$sp(FutureTimeoutSupport.scala:25) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
01:11:07.351UTC [1;31mERROR[0;39m [36mim.actor.server.session.RpcHandler[0;39m [35makka://actor-server/system/sharding/Session/2/6748421325664610582_-8448704640517614576/rpcHandler[0;39m 6748421325664610582
                -8448704640517614576 Failed to process request messageId: 3: Request({ "_name": "RequestRegisterGooglePush","projectId": 43880936595,"token": "FCM_cVuLT_hHcz4:APA91bFwHG3vieEKdavx2F-zqNXbSiGguSyu16HsinSFNklfK_2hEVf34kcRGrBUtIcSlBGF09dNFQn7WX7S_SDQ-Yptd7OfAXJ0BF_QmkvM4LvH5nt0JC9HFst5z0sUSMfOK78d2P8E" })
java.util.concurrent.TimeoutException: Future timed out
	at im.actor.concurrent.package$ExtendedFuture$$anonfun$1.apply(package.scala:14) ~[actor-concurrent_2.11-0.0.27-SNAPSHOT.jar:0.0.27-SNAPSHOT]
	at im.actor.concurrent.package$ExtendedFuture$$anonfun$1.apply(package.scala:14) ~[actor-concurrent_2.11-0.0.27-SNAPSHOT.jar:0.0.27-SNAPSHOT]
	at akka.pattern.FutureTimeoutSupport$$anonfun$after$1.liftedTree1$1(FutureTimeoutSupport.scala:25) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.pattern.FutureTimeoutSupport$$anonfun$after$1.apply$mcV$sp(FutureTimeoutSupport.scala:25) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
01:11:07.356UTC [1;31mERROR[0;39m [36mi.a.s.sequence.SeqUpdatesExtension[0;39m [35mSeqUpdatesExtension(akka://actor-server)[0;39m 
                 AuthSession not found
java.lang.RuntimeException: AuthSession not found
	at im.actor.server.sequence.operations.PushOperations$class.im$actor$server$sequence$operations$PushOperations$class$$$anonfun$14(PushOperations.scala:100) ~[classes/:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
01:11:41.754UTC [1;31mERROR[0;39m [36mim.actor.server.session.RpcHandler[0;39m [35makka://actor-server/system/sharding/Session/2/6748421325664610582_-8448704640517614576/rpcHandler[0;39m 6748421325664610582
                -8448704640517614576 Failed to process request messageId: 3: Request({ "_name": "RequestRegisterGooglePush","projectId": 43880936595,"token": "FCM_cVuLT_hHcz4:APA91bFwHG3vieEKdavx2F-zqNXbSiGguSyu16HsinSFNklfK_2hEVf34kcRGrBUtIcSlBGF09dNFQn7WX7S_SDQ-Yptd7OfAXJ0BF_QmkvM4LvH5nt0JC9HFst5z0sUSMfOK78d2P8E" })
org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "kv_firebase_auth_id_creds_pkey"
  Detail: Key (key)=(6748421325664610582) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2284) ~[postgresql-9.4.1208.jar:9.4.1208]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2003) ~[postgresql-9.4.1208.jar:9.4.1208]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:200) ~[postgresql-9.4.1208.jar:9.4.1208]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:424) ~[postgresql-9.4.1208.jar:9.4.1208]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:161) ~[postgresql-9.4.1208.jar:9.4.1208]
	at org.postgresql.jdbc.PgPreparedStatement.execute(PgPreparedStatement.java:155) ~[postgresql-9.4.1208.jar:9.4.1208]
	at com.zaxxer.hikari.pool.ProxyPreparedStatement.execute(ProxyPreparedStatement.java:44) ~[HikariCP-2.4.6.jar:na]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(HikariProxyPreparedStatement.java) ~[HikariCP-2.4.6.jar:na]
	at slick.jdbc.StatementInvoker.results(StatementInvoker.scala:39) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.jdbc.StatementInvoker.iteratorTo(StatementInvoker.scala:22) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.jdbc.Invoker$class.first(Invoker.scala:31) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.jdbc.StatementInvoker.first(StatementInvoker.scala:16) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.jdbc.StreamingInvokerAction$HeadAction.run(StreamingInvokerAction.scala:52) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.jdbc.StreamingInvokerAction$HeadAction.run(StreamingInvokerAction.scala:51) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:240) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:240) ~[slick_2.11-3.1.1.2.jar:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_131]
	at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_131]
01:11:42.969UTC [1;31mERROR[0;39m [36mi.a.s.sequence.SeqUpdatesExtension[0;39m [35mSeqUpdatesExtension(akka://actor-server)[0;39m 
                 AuthSession not found
java.lang.RuntimeException: AuthSession not found
	at im.actor.server.sequence.operations.PushOperations$class.im$actor$server$sequence$operations$PushOperations$class$$$anonfun$14(PushOperations.scala:100) ~[classes/:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
01:11:42.971UTC [1;31mERROR[0;39m [36mim.actor.server.session.RpcHandler[0;39m [35makka://actor-server/system/sharding/Session/2/6748421325664610582_-8448704640517614576/rpcHandler[0;39m 6748421325664610582
                -8448704640517614576 Failed to process request messageId: 10: Request({ "_name": "RequestRegisterGooglePush","projectId": 43880936595,"token": "FCM_cVuLT_hHcz4:APA91bFwHG3vieEKdavx2F-zqNXbSiGguSyu16HsinSFNklfK_2hEVf34kcRGrBUtIcSlBGF09dNFQn7WX7S_SDQ-Yptd7OfAXJ0BF_QmkvM4LvH5nt0JC9HFst5z0sUSMfOK78d2P8E" })
java.lang.RuntimeException: AuthSession not found
	at im.actor.server.sequence.operations.PushOperations$class.im$actor$server$sequence$operations$PushOperations$class$$$anonfun$14(PushOperations.scala:100) ~[classes/:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
01:11:58.671UTC [1;31mERROR[0;39m [36mi.a.s.sequence.SeqUpdatesExtension[0;39m [35mSeqUpdatesExtension(akka://actor-server)[0;39m 
                 AuthSession not found
java.lang.RuntimeException: AuthSession not found
	at im.actor.server.sequence.operations.PushOperations$class.im$actor$server$sequence$operations$PushOperations$class$$$anonfun$14(PushOperations.scala:100) ~[classes/:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
01:11:58.672UTC [1;31mERROR[0;39m [36mim.actor.server.session.RpcHandler[0;39m [35makka://actor-server/system/sharding/Session/2/6748421325664610582_-8448704640517614576/rpcHandler[0;39m 6748421325664610582
                -8448704640517614576 Failed to process request messageId: 12: Request({ "_name": "RequestRegisterGooglePush","projectId": 43880936595,"token": "FCM_cVuLT_hHcz4:APA91bFwHG3vieEKdavx2F-zqNXbSiGguSyu16HsinSFNklfK_2hEVf34kcRGrBUtIcSlBGF09dNFQn7WX7S_SDQ-Yptd7OfAXJ0BF_QmkvM4LvH5nt0JC9HFst5z0sUSMfOK78d2P8E" })
java.lang.RuntimeException: AuthSession not found
	at im.actor.server.sequence.operations.PushOperations$class.im$actor$server$sequence$operations$PushOperations$class$$$anonfun$14(PushOperations.scala:100) ~[classes/:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
01:12:11.838UTC [1;31mERROR[0;39m [36mi.a.s.sequence.SeqUpdatesExtension[0;39m [35mSeqUpdatesExtension(akka://actor-server)[0;39m 
                 AuthSession not found
java.lang.RuntimeException: AuthSession not found
	at im.actor.server.sequence.operations.PushOperations$class.im$actor$server$sequence$operations$PushOperations$class$$$anonfun$14(PushOperations.scala:100) ~[classes/:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
01:12:11.838UTC [1;31mERROR[0;39m [36mim.actor.server.session.RpcHandler[0;39m [35makka://actor-server/system/sharding/Session/2/6748421325664610582_-8448704640517614576/rpcHandler[0;39m 6748421325664610582
                -8448704640517614576 Failed to process request messageId: 16: Request({ "_name": "RequestRegisterGooglePush","projectId": 43880936595,"token": "FCM_cVuLT_hHcz4:APA91bFwHG3vieEKdavx2F-zqNXbSiGguSyu16HsinSFNklfK_2hEVf34kcRGrBUtIcSlBGF09dNFQn7WX7S_SDQ-Yptd7OfAXJ0BF_QmkvM4LvH5nt0JC9HFst5z0sUSMfOK78d2P8E" })
java.lang.RuntimeException: AuthSession not found
	at im.actor.server.sequence.operations.PushOperations$class.im$actor$server$sequence$operations$PushOperations$class$$$anonfun$14(PushOperations.scala:100) ~[classes/:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[scala-library-2.11.11.jar:na]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.11.jar:na]
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
01:12:14.068UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Request shard [22] home
01:12:14.068UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [WeakUpdatesManagerCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(24 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -18 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]),Map(Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232] -> Vector(24, -18)),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(24 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -18 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 22 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]),Map(Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232] -> Vector(24, -18, 22)),Set(),Set(),false))]
01:12:14.068UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(22,Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232])
01:12:14.068UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 Shard [22] allocated at [Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]]
01:12:14.068UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Host Shard [22] 
01:12:14.068UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Starting shard [22] in region
01:12:14.069UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Shard [22] located at [Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]]
01:12:14.069UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Shard was initialized 22
01:12:14.069UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Deliver [1] buffered messages for shard [22]
01:12:14.069UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager/22[0;39m 
                 Starting entity [6748421325664610582] in shard [22]
 [31mWARN [0;39m [36mcom.zaxxer.hikari.pool.HikariPool[0;39m [35m[0;39m 
                 1m25s679ms186μs17ns - Thread starvation or clock leap detected (housekeeper delta=).
01:13:36.751UTC [1;31mERROR[0;39m [36mim.actor.server.session.RpcHandler[0;39m [35makka://actor-server/system/sharding/Session/2/6748421325664610582_-8448704640517614576/rpcHandler[0;39m 6748421325664610582
                -8448704640517614576 Failed to process request messageId: 56: Request({ "_name": "RequestRegisterGooglePush","projectId": 43880936595,"token": "FCM_cVuLT_hHcz4:APA91bFwHG3vieEKdavx2F-zqNXbSiGguSyu16HsinSFNklfK_2hEVf34kcRGrBUtIcSlBGF09dNFQn7WX7S_SDQ-Yptd7OfAXJ0BF_QmkvM4LvH5nt0JC9HFst5z0sUSMfOK78d2P8E" })
java.util.concurrent.TimeoutException: Future timed out
	at im.actor.concurrent.package$ExtendedFuture$$anonfun$1.apply(package.scala:14) ~[actor-concurrent_2.11-0.0.27-SNAPSHOT.jar:0.0.27-SNAPSHOT]
	at im.actor.concurrent.package$ExtendedFuture$$anonfun$1.apply(package.scala:14) ~[actor-concurrent_2.11-0.0.27-SNAPSHOT.jar:0.0.27-SNAPSHOT]
	at akka.pattern.FutureTimeoutSupport$$anonfun$after$1.liftedTree1$1(FutureTimeoutSupport.scala:25) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.pattern.FutureTimeoutSupport$$anonfun$after$1.apply$mcV$sp(FutureTimeoutSupport.scala:25) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) ~[akka-actor_2.11-2.4.19.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) ~[akka-actor_2.11-2.4.19.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.11.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.11.jar:na]
01:13:49.268UTC [1;31mERROR[0;39m [36mim.actor.server.session.RpcHandler[0;39m [35makka://actor-server/system/sharding/Session/2/6748421325664610582_-8448704640517614576/rpcHandler[0;39m 6748421325664610582
                -8448704640517614576 Failed to process request messageId: 70: Request({ "_name": "RequestRegisterGooglePush","projectId": 43880936595,"token": "FCM_cVuLT_hHcz4:APA91bFwHG3vieEKdavx2F-zqNXbSiGguSyu16HsinSFNklfK_2hEVf34kcRGrBUtIcSlBGF09dNFQn7WX7S_SDQ-Yptd7OfAXJ0BF_QmkvM4LvH5nt0JC9HFst5z0sUSMfOK78d2P8E" })
org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "kv_firebase_auth_id_creds_pkey"
  Detail: Key (key)=(6748421325664610582) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2284) ~[postgresql-9.4.1208.jar:9.4.1208]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2003) ~[postgresql-9.4.1208.jar:9.4.1208]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:200) ~[postgresql-9.4.1208.jar:9.4.1208]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:424) ~[postgresql-9.4.1208.jar:9.4.1208]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:161) ~[postgresql-9.4.1208.jar:9.4.1208]
	at org.postgresql.jdbc.PgPreparedStatement.execute(PgPreparedStatement.java:155) ~[postgresql-9.4.1208.jar:9.4.1208]
	at com.zaxxer.hikari.pool.ProxyPreparedStatement.execute(ProxyPreparedStatement.java:44) ~[HikariCP-2.4.6.jar:na]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(HikariProxyPreparedStatement.java) ~[HikariCP-2.4.6.jar:na]
	at slick.jdbc.StatementInvoker.results(StatementInvoker.scala:39) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.jdbc.StatementInvoker.iteratorTo(StatementInvoker.scala:22) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.jdbc.Invoker$class.first(Invoker.scala:31) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.jdbc.StatementInvoker.first(StatementInvoker.scala:16) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.jdbc.StreamingInvokerAction$HeadAction.run(StreamingInvokerAction.scala:52) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.jdbc.StreamingInvokerAction$HeadAction.run(StreamingInvokerAction.scala:51) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:240) ~[slick_2.11-3.1.1.2.jar:na]
	at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:240) ~[slick_2.11-3.1.1.2.jar:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_131]
	at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_131]
01:14:45.718UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/Session/0[0;39m 
                 Starting entity [-4775611501413901810_-848647742187731935] in shard [0]
01:15:08.179UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager/-18[0;39m 
                 Starting entity [-7409518689298610226] in shard [-18]
01:15:08.180UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Request shard [-26] home
01:15:08.180UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Request shard [-17] home
01:15:08.180UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Request shard [-15] home
01:15:08.180UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Request shard [11] home
01:15:08.180UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Request shard [-23] home
01:15:08.180UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Request shard [12] home
01:15:08.180UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [WeakUpdatesManagerCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(24 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -18 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 22 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]),Map(Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232] -> Vector(24, -18, 22)),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(24 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -18 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 22 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -26 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]),Map(Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232] -> Vector(24, -18, 22, -26)),Set(),Set(),false))]
01:15:08.185UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(-26,Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232])
01:15:08.186UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 Shard [-26] allocated at [Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]]
01:15:08.186UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Host Shard [-26] 
01:15:08.186UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Starting shard [-26] in region
01:15:08.186UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Shard [-26] located at [Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]]
01:15:08.186UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Shard was initialized -26
01:15:08.186UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Deliver [1] buffered messages for shard [-26]
01:15:08.186UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager/-26[0;39m 
                 Starting entity [-6344891477540210874] in shard [-26]
01:15:08.186UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [WeakUpdatesManagerCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(24 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -18 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 22 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -26 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]),Map(Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232] -> Vector(24, -18, 22, -26)),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(-26 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 22 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 24 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -17 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -18 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]),Map(Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232] -> Vector(24, -18, 22, -26, -17)),Set(),Set(),false))]
01:15:08.186UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(-17,Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232])
01:15:08.186UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 Shard [-17] allocated at [Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]]
01:15:08.186UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Host Shard [-17] 
01:15:08.186UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Starting shard [-17] in region
01:15:08.187UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Shard [-17] located at [Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]]
01:15:08.187UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Shard was initialized -17
01:15:08.187UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Deliver [1] buffered messages for shard [-17]
01:15:08.187UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager/-17[0;39m 
                 Starting entity [-1832152895339491601] in shard [-17]
01:15:08.189UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [WeakUpdatesManagerCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(-26 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 22 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 24 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -17 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -18 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]),Map(Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232] -> Vector(24, -18, 22, -26, -17)),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(-26 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -15 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 22 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 24 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -17 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -18 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]),Map(Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232] -> Vector(24, -18, 22, -26, -17, -15)),Set(),Set(),false))]
01:15:08.189UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(-15,Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232])
01:15:08.189UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 Shard [-15] allocated at [Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]]
01:15:08.189UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Host Shard [-15] 
01:15:08.189UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Starting shard [-15] in region
01:15:08.189UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Shard [-15] located at [Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]]
01:15:08.189UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Shard was initialized -15
01:15:08.189UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Deliver [1] buffered messages for shard [-15]
01:15:08.189UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager/-15[0;39m 
                 Starting entity [-3756465188414717839] in shard [-15]
01:15:08.189UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [WeakUpdatesManagerCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(-26 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -15 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 22 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 24 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -17 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -18 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]),Map(Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232] -> Vector(24, -18, 22, -26, -17, -15)),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(-26 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 11 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -15 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 22 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 24 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -17 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -18 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]),Map(Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232] -> Vector(24, -18, 22, -26, -17, -15, 11)),Set(),Set(),false))]
01:15:08.189UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(11,Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232])
01:15:08.189UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 Shard [11] allocated at [Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]]
01:15:08.189UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Host Shard [11] 
01:15:08.189UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Starting shard [11] in region
01:15:08.190UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Shard [11] located at [Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]]
01:15:08.190UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Shard was initialized 11
01:15:08.190UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Deliver [1] buffered messages for shard [11]
01:15:08.190UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [WeakUpdatesManagerCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(-26 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 11 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -15 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 22 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 24 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -17 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -18 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]),Map(Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232] -> Vector(24, -18, 22, -26, -17, -15, 11)),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(-26 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 11 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -15 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 22 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 24 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -17 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -23 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -18 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]),Map(Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232] -> Vector(24, -18, 22, -26, -17, -15, 11, -23)),Set(),Set(),false))]
01:15:08.190UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager/11[0;39m 
                 Starting entity [6673646672371752683] in shard [11]
01:15:08.190UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(-23,Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232])
01:15:08.190UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 Shard [-23] allocated at [Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]]
01:15:08.190UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Host Shard [-23] 
01:15:08.190UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Starting shard [-23] in region
01:15:08.190UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Shard [-23] located at [Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]]
01:15:08.190UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Shard was initialized -23
01:15:08.190UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Deliver [1] buffered messages for shard [-23]
01:15:08.190UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [WeakUpdatesManagerCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(-26 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 11 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -15 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 22 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 24 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -17 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -23 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -18 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]),Map(Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232] -> Vector(24, -18, 22, -26, -17, -15, 11, -23)),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(12 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -26 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 11 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -15 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 22 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], 24 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -17 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -23 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232], -18 -> Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]),Map(Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232] -> Vector(24, -18, 22, -26, -17, -15, 11, -23, 12)),Set(),Set(),false))]
01:15:08.190UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager/-23[0;39m 
                 Starting entity [-7929135468813196535] in shard [-23]
01:15:08.190UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(12,Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232])
01:15:08.190UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManagerCoordinator/singleton/coordinator[0;39m 
                 Shard [12] allocated at [Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]]
01:15:08.190UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Host Shard [12] 
01:15:08.190UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Starting shard [12] in region
01:15:08.190UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Shard [12] located at [Actor[akka://actor-server/system/sharding/WeakUpdatesManager#1164970232]]
01:15:08.190UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Shard was initialized 12
01:15:08.190UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager[0;39m 
                 Deliver [1] buffered messages for shard [12]
01:15:08.191UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/WeakUpdatesManager/12[0;39m 
                 Starting entity [4849077927292132268] in shard [12]
01:15:12.000UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManager[0;39m 
                 Request shard [48] home
01:15:12.000UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManager[0;39m 
                 Request shard [6] home
01:15:12.000UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [SocialManagerCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(),Map(Actor[akka://actor-server/system/sharding/SocialManager#391155454] -> Vector()),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(48 -> Actor[akka://actor-server/system/sharding/SocialManager#391155454]),Map(Actor[akka://actor-server/system/sharding/SocialManager#391155454] -> Vector(48)),Set(),Set(),false))]
01:15:12.000UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManagerCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(48,Actor[akka://actor-server/system/sharding/SocialManager#391155454])
01:15:12.000UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManagerCoordinator/singleton/coordinator[0;39m 
                 Shard [48] allocated at [Actor[akka://actor-server/system/sharding/SocialManager#391155454]]
01:15:12.000UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManager[0;39m 
                 Host Shard [48] 
01:15:12.000UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManager[0;39m 
                 Starting shard [48] in region
01:15:12.001UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManager[0;39m 
                 Shard [48] located at [Actor[akka://actor-server/system/sharding/SocialManager#391155454]]
01:15:12.001UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManager[0;39m 
                 Shard was initialized 48
01:15:12.001UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManager[0;39m 
                 Deliver [1] buffered messages for shard [48]
01:15:12.001UTC [39mDEBUG[0;39m [36makka.cluster.ddata.Replicator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/ddataReplicator[0;39m 
                 Received Update for key [SocialManagerCoordinatorState], old data [Some(DataEnvelope(LWWRegister(State(Map(48 -> Actor[akka://actor-server/system/sharding/SocialManager#391155454]),Map(Actor[akka://actor-server/system/sharding/SocialManager#391155454] -> Vector(48)),Set(),Set(),false)),Map()))], new data [LWWRegister(State(Map(48 -> Actor[akka://actor-server/system/sharding/SocialManager#391155454], 6 -> Actor[akka://actor-server/system/sharding/SocialManager#391155454]),Map(Actor[akka://actor-server/system/sharding/SocialManager#391155454] -> Vector(48, 6)),Set(),Set(),false))]
01:15:12.001UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManagerCoordinator/singleton/coordinator[0;39m 
                 The coordinator state was successfully updated with ShardHomeAllocated(6,Actor[akka://actor-server/system/sharding/SocialManager#391155454])
01:15:12.001UTC [39mDEBUG[0;39m [36ma.c.sharding.DDataShardCoordinator[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManagerCoordinator/singleton/coordinator[0;39m 
                 Shard [6] allocated at [Actor[akka://actor-server/system/sharding/SocialManager#391155454]]
01:15:12.001UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManager/48[0;39m 
                 Starting entity [1955255348] in shard [48]
01:15:12.001UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManager[0;39m 
                 Host Shard [6] 
01:15:12.001UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManager[0;39m 
                 Starting shard [6] in region
01:15:12.001UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManager[0;39m 
                 Shard [6] located at [Actor[akka://actor-server/system/sharding/SocialManager#391155454]]
01:15:12.001UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManager[0;39m 
                 Shard was initialized 6
01:15:12.001UTC [39mDEBUG[0;39m [36makka.cluster.sharding.ShardRegion[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManager[0;39m 
                 Deliver [1] buffered messages for shard [6]
01:15:12.001UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/SocialManager/6[0;39m 
                 Starting entity [266296106] in shard [6]
01:15:12.052UTC [31mWARN [0;39m [36mi.a.s.p.google.FirebasePushExtension[0;39m [35mFirebasePushExtension(akka://actor-server)[0;39m 
                 Key not found for projectId: 43880936595
01:15:12.110UTC [39mDEBUG[0;39m [36mi.a.s.push.google.GooglePushDelivery[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/gcm-delivery[0;39m 
                 Trying to deliver google push. Queue size: 0, totalDemand: 14
01:15:12.110UTC [31mWARN [0;39m [36mi.a.s.p.google.FirebasePushExtension[0;39m [35mFirebasePushExtension(akka://actor-server)[0;39m 
                 Key not found for projectId: 43880936595
01:15:12.440UTC [39mDEBUG[0;39m [36mi.a.s.push.google.GooglePushDelivery[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/gcm-delivery[0;39m 
                 Trying to deliver google push. Queue size: 0, totalDemand: 13
01:15:12.440UTC [31mWARN [0;39m [36mi.a.s.p.google.FirebasePushExtension[0;39m [35mFirebasePushExtension(akka://actor-server)[0;39m 
                 Key not found for projectId: 43880936595
01:15:13.441UTC [39mDEBUG[0;39m [36mi.a.s.push.google.DeliveryStream[0;39m [35mDeliveryStream(akka://actor-server)[0;39m 
                 GCM: Successfully delivered: Delivery(GooglePushMessage(f7BGZuo3n-8:APA91bH4sdf65fBOi1zQuFoGBTbnbXjxkf3BvcgHtXE6f0CB3o6DqlO6CAwTBE-p7UHkHASsvBvTkM1QbHHrMKOfGwpI_Y5e16YelS9DOZwwZe6JZdbLrnH_tDGP1wcc0iYpqv0Ex1Y-,Some(seq-invisible-1955255348),Some(Map(seq -> 4, _authId -> -4775611501413901810)),None),AIzaSyAliS4bfKy8GucSRX1pnCLJHM5HroHPpLk)
01:15:13.863UTC [39mDEBUG[0;39m [36mi.a.s.push.google.DeliveryStream[0;39m [35mDeliveryStream(akka://actor-server)[0;39m 
                 GCM: Successfully delivered: Delivery(GooglePushMessage(f7BGZuo3n-8:APA91bH4sdf65fBOi1zQuFoGBTbnbXjxkf3BvcgHtXE6f0CB3o6DqlO6CAwTBE-p7UHkHASsvBvTkM1QbHHrMKOfGwpI_Y5e16YelS9DOZwwZe6JZdbLrnH_tDGP1wcc0iYpqv0Ex1Y-,Some(seq-invisible-1955255348),Some(Map(seq -> 6, _authId -> -4775611501413901810)),None),AIzaSyAliS4bfKy8GucSRX1pnCLJHM5HroHPpLk)
01:15:38.525UTC [31mWARN [0;39m [36mi.a.s.p.google.FirebasePushExtension[0;39m [35mFirebasePushExtension(akka://actor-server)[0;39m 
                 Key not found for projectId: 43880936595
01:15:38.791UTC [39mDEBUG[0;39m [36mi.a.s.push.google.GooglePushDelivery[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/gcm-delivery[0;39m 
                 Trying to deliver google push. Queue size: 0, totalDemand: 12
01:15:38.791UTC [31mWARN [0;39m [36mi.a.s.p.google.FirebasePushExtension[0;39m [35mFirebasePushExtension(akka://actor-server)[0;39m 
                 Key not found for projectId: 43880936595
01:15:40.180UTC [39mDEBUG[0;39m [36mi.a.s.push.google.DeliveryStream[0;39m [35mDeliveryStream(akka://actor-server)[0;39m 
                 GCM: Successfully delivered: Delivery(GooglePushMessage(f7BGZuo3n-8:APA91bH4sdf65fBOi1zQuFoGBTbnbXjxkf3BvcgHtXE6f0CB3o6DqlO6CAwTBE-p7UHkHASsvBvTkM1QbHHrMKOfGwpI_Y5e16YelS9DOZwwZe6JZdbLrnH_tDGP1wcc0iYpqv0Ex1Y-,Some(seq-invisible-1955255348),Some(Map(seq -> 8, _authId -> -4775611501413901810)),None),AIzaSyAliS4bfKy8GucSRX1pnCLJHM5HroHPpLk)
01:16:52.940UTC [31mWARN [0;39m [36mi.a.s.p.google.FirebasePushExtension[0;39m [35mFirebasePushExtension(akka://actor-server)[0;39m 
                 Key not found for projectId: 43880936595
01:17:27.539UTC [39mDEBUG[0;39m [36makka.cluster.sharding.Shard[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/system/sharding/Session/2[0;39m 
                 Starting entity [6748421325664610582_7704345586316909424] in shard [2]
01:17:34.805UTC [31mWARN [0;39m [36mi.a.s.p.google.FirebasePushExtension[0;39m [35mFirebasePushExtension(akka://actor-server)[0;39m 
                 Key not found for projectId: 43880936595
01:17:34.805UTC [39mDEBUG[0;39m [36mi.a.s.push.google.GooglePushDelivery[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/gcm-delivery[0;39m 
                 Trying to deliver google push. Queue size: 0, totalDemand: 11
01:17:35.549UTC [39mDEBUG[0;39m [36mi.a.s.push.google.DeliveryStream[0;39m [35mDeliveryStream(akka://actor-server)[0;39m 
                 GCM: Successfully delivered: Delivery(GooglePushMessage(f7BGZuo3n-8:APA91bH4sdf65fBOi1zQuFoGBTbnbXjxkf3BvcgHtXE6f0CB3o6DqlO6CAwTBE-p7UHkHASsvBvTkM1QbHHrMKOfGwpI_Y5e16YelS9DOZwwZe6JZdbLrnH_tDGP1wcc0iYpqv0Ex1Y-,Some(seq-invisible-1955255348),Some(Map(seq -> 10, _authId -> -4775611501413901810)),None),AIzaSyAliS4bfKy8GucSRX1pnCLJHM5HroHPpLk)
01:17:48.030UTC [39mDEBUG[0;39m [36mi.a.s.push.google.GooglePushDelivery[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/gcm-delivery[0;39m 
                 Trying to deliver google push. Queue size: 0, totalDemand: 10
01:17:48.030UTC [31mWARN [0;39m [36mi.a.s.p.google.FirebasePushExtension[0;39m [35mFirebasePushExtension(akka://actor-server)[0;39m 
                 Key not found for projectId: 43880936595
01:17:48.071UTC [31mWARN [0;39m [36mi.a.s.p.google.FirebasePushExtension[0;39m [35mFirebasePushExtension(akka://actor-server)[0;39m 
                 Key not found for projectId: 43880936595
01:17:50.243UTC [39mDEBUG[0;39m [36mi.a.s.push.google.DeliveryStream[0;39m [35mDeliveryStream(akka://actor-server)[0;39m 
                 GCM: Successfully delivered: Delivery(GooglePushMessage(f7BGZuo3n-8:APA91bH4sdf65fBOi1zQuFoGBTbnbXjxkf3BvcgHtXE6f0CB3o6DqlO6CAwTBE-p7UHkHASsvBvTkM1QbHHrMKOfGwpI_Y5e16YelS9DOZwwZe6JZdbLrnH_tDGP1wcc0iYpqv0Ex1Y-,Some(seq-invisible-1955255348),Some(Map(seq -> 11, _authId -> -4775611501413901810)),None),AIzaSyAliS4bfKy8GucSRX1pnCLJHM5HroHPpLk)
01:17:52.371UTC [31mWARN [0;39m [36mi.a.s.p.google.FirebasePushExtension[0;39m [35mFirebasePushExtension(akka://actor-server)[0;39m 
                 Key not found for projectId: 43880936595
01:17:52.681UTC [31mWARN [0;39m [36mi.a.s.p.google.FirebasePushExtension[0;39m [35mFirebasePushExtension(akka://actor-server)[0;39m 
                 Key not found for projectId: 43880936595
01:17:52.681UTC [39mDEBUG[0;39m [36mi.a.s.push.google.GooglePushDelivery[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/gcm-delivery[0;39m 
                 Trying to deliver google push. Queue size: 0, totalDemand: 9
01:17:52.681UTC [39mDEBUG[0;39m [36mi.a.s.push.google.GooglePushDelivery[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/gcm-delivery[0;39m 
                 Trying to deliver google push. Queue size: 0, totalDemand: 16, subscriber requests 8 elements
01:17:53.814UTC [39mDEBUG[0;39m [36mi.a.s.push.google.DeliveryStream[0;39m [35mDeliveryStream(akka://actor-server)[0;39m 
                 GCM: Successfully delivered: Delivery(GooglePushMessage(f7BGZuo3n-8:APA91bH4sdf65fBOi1zQuFoGBTbnbXjxkf3BvcgHtXE6f0CB3o6DqlO6CAwTBE-p7UHkHASsvBvTkM1QbHHrMKOfGwpI_Y5e16YelS9DOZwwZe6JZdbLrnH_tDGP1wcc0iYpqv0Ex1Y-,Some(seq-invisible-1955255348),Some(Map(seq -> 13, _authId -> -4775611501413901810)),None),AIzaSyAliS4bfKy8GucSRX1pnCLJHM5HroHPpLk)
01:17:56.481UTC [39mDEBUG[0;39m [36mi.a.s.push.google.GooglePushDelivery[0;39m [35makka.tcp://actor-server@192.168.1.3:2552/user/gcm-delivery[0;39m 
                 Trying to deliver google push. Queue size: 0, totalDemand: 16
01:17:56.481UTC [31mWARN [0;39m [36mi.a.s.p.google.FirebasePushExtension[0;39m [35mFirebasePushExtension(akka://actor-server)[0;39m 
                 Key not found for projectId: 43880936595
01:17:56.520UTC [31mWARN [0;39m [36mi.a.s.p.google.FirebasePushExtension[0;39m [35mFirebasePushExtension(akka://actor-server)[0;39m 
                 Key not found for projectId: 43880936595
01:17:57.347UTC [39mDEBUG[0;39m [36mi.a.s.push.google.DeliveryStream[0;39m [35mDeliveryStream(akka://actor-server)[0;39m 
                 GCM: Successfully delivered: Delivery(GooglePushMessage(f7BGZuo3n-8:APA91bH4sdf65fBOi1zQuFoGBTbnbXjxkf3BvcgHtXE6f0CB3o6DqlO6CAwTBE-p7UHkHASsvBvTkM1QbHHrMKOfGwpI_Y5e16YelS9DOZwwZe6JZdbLrnH_tDGP1wcc0iYpqv0Ex1Y-,Some(seq-invisible-1955255348),Some(Map(seq -> 14, _authId -> -4775611501413901810)),None),AIzaSyAliS4bfKy8GucSRX1pnCLJHM5HroHPpLk)
01:18:22.923UTC [31mWARN [0;39m [36mi.a.s.p.google.FirebasePushExtension[0;39m [35mFirebasePushExtension(akka://actor-server)[0;39m 
                 Key not found for projectId: 43880936595
01:18:33.309UTC [31mWARN [0;39m [36mi.a.s.p.google.FirebasePushExtension[0;39m [35mFirebasePushExtension(akka://actor-server)[0;39m 
                 Key not found for projectId: 43880936595
01:18:41.011UTC [31mWARN [0;39m [36mi.a.s.p.google.FirebasePushExtension[0;39m [35mFirebasePushExtension(akka://actor-server)[0;39m 
                 Key not found for projectId: 43880936595
01:18:45.012UTC [31mWARN [0;39m [36mi.a.s.p.google.FirebasePushExtension[0;39m [35mFirebasePushExtension(akka://actor-server)[0;39m 
                 Key not found for projectId: 43880936595
01:19:46.138UTC [1;31mERROR[0;39m [36mi.a.s.push.google.DeliveryStream[0;39m [35mDeliveryStream(akka://actor-server)[0;39m 
                 Firebase: Failure in stream
akka.stream.AbruptTerminationException: Processor actor [Actor[akka://actor-server/user/StreamSupervisor-18/flow-29-3-foreachSink-foreachSink-ignoreSink#-1712810778]] terminated abruptly
